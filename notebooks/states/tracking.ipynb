{"cells": [{"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Tracking\n", "At various stages, you are likely going to want to keep track of certain key metrics, like the number of tokens used, the latency of parts of your pipeline, and metrics to measure performance. In this notebook, we will explore how to track some of this information, with a focus on:\n", "\n", "- tracking token usage and cost, which would form the basis of a cost model, or tracking user activity\n", "- tracking latency of parts of your pipeline, which would help you identify bottlenecks and optimize your pipeline\n", "\n", "These two methods of tracking may not occur at the same place...\n", "\n", "## Tracking Token Usage and Cost\n", "Every time we make a request to the OpenAI API, we get a `response` object that looks like this:\n", "\n", "```python\n", "ChatCompletion(\n", "\u2502   id='chatcmpl-APr4B3kQAhDr142TmpMAZCxcNrhjl',\n", "\u2502   choices=[\n", "\u2502   \u2502   Choice(\n", "\u2502   \u2502   \u2502   finish_reason='stop',\n", "\u2502   \u2502   \u2502   index=0,\n", "\u2502   \u2502   \u2502   logprobs=None,\n", "\u2502   \u2502   \u2502   message=ChatCompletionMessage(\n", "\u2502   \u2502   \u2502   \u2502   content='\"Titanic\" is a romantic drama that follows the ill-fated love story between a young woman from a wealthy family and a penniless artist aboard the doomed RMS Titanic, exploring themes of class struggle and tragedy amidst the ship\\'s tragic sinking.',\n", "\u2502   \u2502   \u2502   \u2502   refusal=None,\n", "\u2502   \u2502   \u2502   \u2502   role='assistant',\n", "\u2502   \u2502   \u2502   \u2502   audio=None,\n", "\u2502   \u2502   \u2502   \u2502   function_call=None,\n", "\u2502   \u2502   \u2502   \u2502   tool_calls=None\n", "\u2502   \u2502   \u2502   )\n", "\u2502   \u2502   )\n", "\u2502   ],\n", "\u2502   created=1730725551,\n", "\u2502   model='gpt-4o-mini-2024-07-18',\n", "\u2502   object='chat.completion',\n", "\u2502   service_tier=None,\n", "\u2502   system_fingerprint='fp_8bfc6a7dc2',\n", "\u2502   usage=CompletionUsage(\n", "\u2502   \u2502   completion_tokens=49,\n", "\u2502   \u2502   prompt_tokens=26,\n", "\u2502   \u2502   total_tokens=75,\n", "\u2502   \u2502   completion_tokens_details=CompletionTokensDetails(\n", "\u2502   \u2502   \u2502   audio_tokens=None,\n", "\u2502   \u2502   \u2502   reasoning_tokens=0\n", "\u2502   \u2502   ),\n", "\u2502   \u2502   prompt_tokens_details=PromptTokensDetails(\n", "\u2502   \u2502   \u2502   audio_tokens=None,\n", "\u2502   \u2502   \u2502   cached_tokens=0\n", "\u2502   \u2502   )\n", "\u2502   )\n", "```\n", "\n", "In this object contains a chat `id`, the `model`, the `messages` recieved, and the `usage` object. The `usage` object contains the number of `completion_tokens`, `prompt_tokens`, and `total_tokens` used in the request. This information can be used to track the number of tokens used by a user, and therefore cost of the request.\n", "\n", "\n", "#### So how do we track this information?\n", "We will focus on a fairly simple method, which will essentially just wrap the OpenAI API in a custom class that will perform this logging for us, and give the ability to turn it on and off.\n", "\n", "Here is an example of how we call the OpenAI API to generate the output above:\n", "\n", "```python\n", "from openai import OpenAI\n", "\n", "client = OpenAI()\n", "response = client.chat.completions.create(\n", "    model='gpt-4o-mini',\n", "    messages=[\n", "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n", "        {\"role\": \"user\", \"content\": \"Give me a one sentence summary of Titanic.\"},\n", "    ],\n", "    temperature=0.7,\n", ")\n", "```"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["What we want to do is something like:\n", "    \n", "```python\n", "client = OpenAIWrapper(\n", "    log=True,\n", "    log_dir='./logs',\n", "    experiment_name='my_experiment',\n", "    api_key='sk-123',\n", "    costs_config='./costs.yaml',\n", ")\n", "\n", "response = client.completion(\n", "    model='gpt-4o-mini',\n", "    messages=[\n", "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n", "        {\"role\": \"user\", \"content\": \"Give me a one sentence summary of Titanic.\"},\n", "    ],\n", "    temperature=0.7,\n", ")\n", "```"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["The experience is basically identical, after the `client` instantiation. So what does the `OpenAIWrapper` class look like?\n", "\n", "We want to supply a logging directory, a config file that contains costs (and perhaps other information), and the name of our session."]}, {"cell_type": "code", "execution_count": 2, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [{"data": {"text/plain": ["True"]}, "execution_count": 2, "metadata": {}, "output_type": "execute_result"}], "source": ["from datetime import datetime\n", "import json\n", "from pathlib import Path\n", "import time\n", "import uuid\n", "from typing import Any\n", "from openai import OpenAI\n", "import yaml\n", "\n", "api_key = \"abcd1234\""]}, {"cell_type": "code", "execution_count": 3, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["class OpenAIWrapper:\n", "    \"\"\"A simple wrapper for the OpenAI API with usage tracking.\n", "    \"\"\"\n", "    def __init__(\n", "        self,\n", "        log: bool = True,\n", "        output_dir: str = \"./logs\",\n", "        experiment_name: str = None,\n", "        api_key: str = None,\n", "        costs_dir: str = None\n", "    ):\n", "        \"\"\"\n", "        Initialize the OpenAI wrapper.\n", "        \n", "        Args:\n", "            output_dir: Directory where logs will be stored\n", "            experiment_name: Optional name for this experiment\n", "            api_key: OpenAI API key (optional, will use environment variable if not provided)\n", "        \"\"\"\n", "        self.client = OpenAI(api_key=api_key)\n", "        self.log = log\n", "\n", "        # don't log if log is False\n", "        if self.log:\n", "            self.conversation_id = str(uuid.uuid4())\n", "            self.session_start = datetime.now()\n", "            self.experiment_name = experiment_name\n", "            \n", "            # Create session directory using date and UUID\n", "            date_str = self.session_start.strftime(\"%Y-%m-%d\")\n", "            self.session_dir = Path(output_dir) / self.experiment_name / date_str\n", "            self.session_dir.mkdir(parents=True, exist_ok=True)\n", "            # load costs yaml\n", "            if costs_dir:\n", "                with open(costs_dir, 'r') as f:\n", "                    self.costs = yaml.safe_load(f)\n", "            else:\n", "                self.costs = {}"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Have a look through this initializer and make sure you understand each line. We can see that we are creating a different folder for each \"conversation\" that we have to track metrics.\n", "\n", "The one thing that might be confusing is the `costs` attribute. This is a dictionary that contains the costs of each token type. This is a simple way to track costs, but you could also use a more complex system that tracks costs based on the model used, or other factors. The `costs.yaml` file looks like this:\n", "\n", "```yaml\n", "gpt-4o:\n", "  input: 0.0025\n", "  output: 0.01\n", "gpt-4o-mini:\n", "  input: 0.0015\n", "  output: 0.006\n", "  ```"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Now we need to write the completion method."]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["```python\n", "def completion(\n", "        self,\n", "        messages: list[dict[str, str]],\n", "        model: str = \"gpt-4o-mini\",\n", "        **kwargs\n", "    ) -> Any:\n", "    \"\"\"Call the OpenAI API for completion and log the usage. All valid OpenAI parameters can be passed as keyword arguments.\n", "\n", "    Args:\n", "        messages (list[dict[str, str]]): List of messages to send to the model\n", "        model (str, optional): _description_. Defaults to \"gpt-4o-mini\".\n", "\n", "    Returns:\n", "        Any: _description_\n", "    \"\"\"\n", "    start_time = time.time()\n", "\n", "    try:\n", "        # Make API call\n", "        response = self.client.chat.completions.create(\n", "            model=model,\n", "            messages=messages,\n", "            **kwargs\n", "        )\n", "\n", "        if self.log:\n", "\n", "            self._log_metadata()\n", "            \n", "            # Log request\n", "            self._write_log('interactions', {\n", "                'request_id': response.id,\n", "                'timestamp': datetime.now().isoformat(),\n", "                'model': model,\n", "                'parameters': kwargs,\n", "            })\n", "\n", "            costs = self._calculate_cost(\n", "                model,\n", "                response.usage.prompt_tokens,\n", "                response.usage.completion_tokens\n", "            )\n", "            \n", "            # Log usage\n", "            self._write_log('token_usage', {\n", "                'request_id': response.id,\n", "                'timestamp': datetime.now().isoformat(),\n", "                'model': model,\n", "                'prompt_tokens': response.usage.prompt_tokens,\n", "                'completion_tokens': response.usage.completion_tokens,\n", "                'total_tokens': response.usage.total_tokens,\n", "                'duration_seconds': time.time() - start_time,\n", "                'costs': costs\n", "            })\n", "\n", "        return response\n", "        \n", "    except Exception as e:\n", "        # Log error and re-raise\n", "        if self.log:\n", "            self._write_log('errors', {\n", "                'request_id': self.conversation_id,\n", "                'timestamp': datetime.now().isoformat(),\n", "                'error_type': type(e).__name__,\n", "                'error_message': str(e),\n", "                'duration_seconds': time.time() - start_time,\n", "            })\n", "        raise\n", "```"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Although this might look complicated, it is actually quite simple. The key part is:\n", "\n", "```python\n", "response = self.client.chat.completions.create(\n", "    model=model,\n", "    messages=messages,\n", "    **kwargs\n", ")\n", "```\n", "\n", "We pass in the `model` and the `messages`, the only two arguments required by the API, and then any other `**kwargs`. This is a common Python pattern that allows us to pass in any number of keyword arguments to the function, and ensures that we get a consistent experience between the regular OpenAI API, and our wrapped version.\n", "\n", "The rest of the method is just logging the information that we want to log.\n", "\n", "#### Metadata\n", "This method logs basic information about the session, like the `conversation_id`, the `session_start`, and the `experiment_name`.\n", "```python\n", "def _log_metadata(self):\n", "    \"\"\"Log metadata about the session.\"\"\"\n", "    metadata = {\n", "        'conversation_id': self.conversation_id,\n", "        'session_start': self.session_start.isoformat(),\n", "        'experiment_name': self.experiment_name,\n", "    }\n", "    self._write_log('metadata', metadata)\n", "```\n", "\n", "#### Token usage\n", "First we have to calculate the tokens used and the cost of those tokens. This is done with the `_calculate_cost` method, which looks like this:\n", "\n", "```python\n", "def _calculate_cost(self, model: str, prompt_tokens: int, completion_tokens: int) -> dict[str, float]:\n", "    \"\"\"Calculate the cost of the API call.\"\"\"\n", "    model_cost = self.costs.get(model, {'input': 0, 'output': 0})\n", "    input_cost = model_cost['input'] * prompt_tokens / 1000.0\n", "    output_cost = model_cost['output'] * completion_tokens / 1000.0\n", "    total_cost = (input_cost + output_cost)\n", "    return {'input': input_cost, 'output': output_cost, 'total': total_cost}\n", "```"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["And then we log this information using the same `_write_log` method that we have been using.\n", "\n", "There is also some error handling in there as well, which logs any errors that occur during the API call."]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["The whole class then looks like this:"]}, {"cell_type": "code", "execution_count": 4, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["class OpenAIWrapper:\n", "    \"\"\"A simple wrapper for the OpenAI API with usage tracking.\n", "    \"\"\"\n", "    def __init__(\n", "        self,\n", "        log = True,\n", "        output_dir: str = './logs',\n", "        experiment_name: str = None,\n", "        api_key: str = None,\n", "        costs_dir: str = None\n", "    ):\n", "        \"\"\"\n", "        Initialize the OpenAI wrapper.\n", "        \n", "        Args:\n", "            output_dir: Directory where logs will be stored\n", "            experiment_name: Optional name for this experiment\n", "            api_key: OpenAI API key (optional, will use environment variable if not provided)\n", "        \"\"\"\n", "        self.client = OpenAI(api_key=api_key)\n", "\n", "        self.log = log\n", "\n", "        if self.log:\n", "            self.conversation_id = str(uuid.uuid4())\n", "            self.session_start = datetime.now()\n", "            self.experiment_name = experiment_name\n", "            \n", "            # Create session directory using date and UUID\n", "            date_str = self.session_start.strftime(\"%Y-%m-%d\")\n", "            self.session_dir = Path(output_dir) / self.experiment_name / date_str\n", "            self.session_dir.mkdir(parents=True, exist_ok=True)\n", "            # load costs yaml\n", "            if costs_dir:\n", "                with open(costs_dir, 'r') as f:\n", "                    self.costs = yaml.safe_load(f)\n", "            else:\n", "                self.costs = {}\n", "    \n", "\n", "    def _log_metadata(self):\n", "        \"\"\"Log metadata about the session.\"\"\"\n", "        metadata = {\n", "            'conversation_id': self.conversation_id,\n", "            'session_start': self.session_start.isoformat(),\n", "            'experiment_name': self.experiment_name,\n", "        }\n", "        self._write_log('metadata', metadata)\n", "    \n", "\n", "    def _write_log(self, log_type: str, data: dict):\n", "        \"\"\"Write a log entry to the appropriate file.\"\"\"\n", "        file_path = self.session_dir / f\"{log_type}.jsonl\"\n", "        with open(file_path, 'a') as f:\n", "            json.dump(data, f)\n", "            f.write('\\n')\n", "    \n", "    \n", "    def _calculate_cost(self, model: str, prompt_tokens: int, completion_tokens: int) -> dict[str, float]:\n", "        \"\"\"Calculate the cost of the API call.\"\"\"\n", "        model_cost = self.costs.get(model, {'input': 0, 'output': 0})\n", "        input_cost = model_cost['input'] * prompt_tokens / 1000.0\n", "        output_cost = model_cost['output'] * completion_tokens / 1000.0\n", "        total_cost = (input_cost + output_cost)\n", "        return {'input': input_cost, 'output': output_cost, 'total': total_cost}\n", "\n", "\n", "    def completion(\n", "        self,\n", "        messages: list[dict[str, str]],\n", "        model: str = \"gpt-4o-mini\",\n", "        **kwargs\n", "    ) -> Any:\n", "        \"\"\"Call the OpenAI API for completion and log the usage. All valid OpenAI parameters can be passed as keyword arguments.\n", "\n", "        Args:\n", "            messages (list[dict[str, str]]): List of messages to send to the model\n", "            model (str, optional): _description_. Defaults to \"gpt-4o-mini\".\n", "\n", "        Returns:\n", "            Any: _description_\n", "        \"\"\"\n", "        start_time = time.time()\n", "    \n", "        try:\n", "            # Make API call\n", "            response = self.client.chat.completions.create(\n", "                model=model,\n", "                messages=messages,\n", "                **kwargs\n", "            )\n", "\n", "            if self.log:\n", "                # Log request\n", "                self._write_log('interactions', {\n", "                    'request_id': response.id,\n", "                    'timestamp': datetime.now().isoformat(),\n", "                    'model': model,\n", "                    'parameters': kwargs,\n", "                })\n", "\n", "                costs = self._calculate_cost(\n", "                    model,\n", "                    response.usage.prompt_tokens,\n", "                    response.usage.completion_tokens\n", "                )\n", "                \n", "                # Log usage\n", "                self._write_log('token_usage', {\n", "                    'request_id': response.id,\n", "                    'timestamp': datetime.now().isoformat(),\n", "                    'model': model,\n", "                    'prompt_tokens': response.usage.prompt_tokens,\n", "                    'completion_tokens': response.usage.completion_tokens,\n", "                    'total_tokens': response.usage.total_tokens,\n", "                    'duration_seconds': time.time() - start_time,\n", "                    'costs': costs\n", "                })\n", "\n", "                self._log_metadata()\n", "            \n", "            return response\n", "            \n", "        except Exception as e:\n", "            # Log error and re-raise\n", "            if self.log:\n", "                self._write_log('errors', {\n", "                    'request_id': self.conversation_id,\n", "                    'timestamp': datetime.now().isoformat(),\n", "                    'error_type': type(e).__name__,\n", "                    'error_message': str(e),\n", "                    'duration_seconds': time.time() - start_time,\n", "                })\n", "            raise\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["We can now simply instantiate the client and use it as we would the regular OpenAI API, but with the added benefit of logging!"]}, {"cell_type": "code", "execution_count": 5, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["client = OpenAIWrapper(\n", "    log=True,\n", "    output_dir=\"./logs\",\n", "    experiment_name=\"log-test\",\n", "    costs_dir=\"./costs.yaml\", \n", "    api_key = api_key\n", ")"]}, {"cell_type": "code", "execution_count": 6, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["\"Titanic\" is a romantic drama that follows the ill-fated voyage of the RMS Titanic, focusing on the love story between a wealthy young woman and a poor artist, set against the backdrop of the ship's tragic sinking.\n"]}], "source": ["response = client.completion(\n", "    model='gpt-4o-mini',\n", "    messages=[\n", "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n", "        {\"role\": \"user\", \"content\": \"Give me a one sentence summary of Titanic.\"},\n", "    ],\n", "    temperature=0.7,\n", ")\n", "\n", "print(response.choices[0].message.content)"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["We should end up with a directory structure that looks like this:\n", "\n", "```plaintext\n", "log-test\n", "    \u2514\u2500\u2500 YYYY-MM-DD\n", "        \u251c\u2500\u2500 interactions.jsonl\n", "        \u251c\u2500\u2500 metadata.jsonl\n", "        \u2514\u2500\u2500 token_usage.jsonl\n", "```\n", "Where `YYYY-MM-DD` corresponds to the date on which you're running this code."]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["We can then look at token usage:"]}, {"cell_type": "code", "execution_count": 10, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# load jsonl file\n", "with open('./logs/log-test/2024-11-09/token_usage.jsonl', 'r') as f:\n", "    token_usage = json.load(f)"]}, {"cell_type": "code", "execution_count": 11, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [{"data": {"text/plain": ["{'request_id': 'chatcmpl-AQftQoPvtHsKjn39khEx4SOgd32Sw',\n", " 'timestamp': '2024-11-06T19:22:09.897551',\n", " 'model': 'gpt-4o-mini',\n", " 'prompt_tokens': 26,\n", " 'completion_tokens': 46,\n", " 'total_tokens': 72,\n", " 'duration_seconds': 1.8626511096954346,\n", " 'costs': {'input': 3.9e-05,\n", "  'output': 0.00027600000000000004,\n", "  'total': 0.00031500000000000007}}"]}, "execution_count": 11, "metadata": {}, "output_type": "execute_result"}], "source": ["token_usage"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["There are other things you can do to add functionality. For example:\n", "\n", "- You might want to have a single metadata file for all sessions with an experiment name.\n", "- You can write some functions to turn logging on or off, or perhaps certain parts of it.\n", "- You could set up warnings for when you are getting close to your token limit."]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Latency Tracking\n", "Tracking latency is a little more complicated. We can't just wrap the OpenAI API in a class and log the time it takes to make the request, because the latency of the request is not just the time it takes to make the request, but all the other stuff that needs to happen.\n", "\n", "In addition, we might want to track the latency of different parts of our pipeline, like the time it takes to generate a prompt, the time it takes to generate a completion, and the time it takes to send the response back to the user.\n", "\n", "We will therefore use a decorator to track the latency of different parts of our pipeline. A decorator is a function that takes another function as an argument, and returns a new function that wraps the original function. This allows us to add functionality to the original function without modifying it.\n", "\n", "Confused!? Don't worry, let's look at an example."]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Decorators\n", "In Python, functions are objects:"]}, {"cell_type": "code", "execution_count": 16, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["def greet(name):\n", "    return f\"Hello, {name}!\""]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["We can assign functions to variables."]}, {"cell_type": "code", "execution_count": 19, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [{"data": {"text/plain": ["'Hello, Alice!'"]}, "execution_count": 19, "metadata": {}, "output_type": "execute_result"}], "source": ["my_function = greet\n", "my_function(\"Alice\")"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["We can also pass functions as arguments."]}, {"cell_type": "code", "execution_count": 20, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [{"data": {"text/plain": ["'Hello, Bob!'"]}, "execution_count": 20, "metadata": {}, "output_type": "execute_result"}], "source": ["def apply_function(func, value):\n", "    return func(value)\n", "\n", "apply_function(greet, \"Bob\")"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["We can also make functions that return functions"]}, {"cell_type": "code", "execution_count": 21, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["def make_greeting(greeting_word):\n", "    def greet(name):\n", "        return f\"{greeting_word}, {name}!\"\n", "    return greet"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["So now we can create new functions with different greetings"]}, {"cell_type": "code", "execution_count": 22, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Hello, Alice!\n", "Hi, Bob!\n"]}], "source": ["say_hello = make_greeting(\"Hello\")\n", "say_hi = make_greeting(\"Hi\")\n", "\n", "print(say_hello(\"Alice\"))\n", "print(say_hi(\"Bob\"))"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Here is an example of a simple decorator:"]}, {"cell_type": "code", "execution_count": 24, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["def my_decorator(func):\n", "    def wrapper():\n", "        print(\"Before the function runs\")\n", "        func()\n", "        print(\"After the function runs\")\n", "    return wrapper"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["We can manually apply the decorator to a function like this:"]}, {"cell_type": "code", "execution_count": 25, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Before the function runs\n", "Hello!\n", "After the function runs\n"]}], "source": ["def say_hello():\n", "    print(\"Hello!\")\n", "\n", "\n", "decorated_hello = my_decorator(say_hello)\n", "decorated_hello()"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["So we had our original `say_hello` function and then we created a new function `my_decorator` that wraps the original function and adds some functionality - in this case just some print statements."]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["We can also use use the `@` symbol to apply the decorator to the function:"]}, {"cell_type": "code", "execution_count": 26, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Before the function runs\n", "Goodbye!\n", "After the function runs\n"]}], "source": ["@my_decorator\n", "def say_goodbye():\n", "    print(\"Goodbye!\")\n", "\n", "say_goodbye()"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["We can also pass in arguments to the decorator. Here is a basic decorator:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Basic decorator example:\n", "Calling add\n", "5\n"]}], "source": ["def log_call(func):\n", "    def wrapper(*args, **kwargs):\n", "        print(f\"Calling {func.__name__}\")\n", "        return func(*args, **kwargs)\n", "    return wrapper\n", "\n", "@log_call\n", "def add(a, b):\n", "    return a + b\n", "\n", "print(\"Basic decorator example:\")\n", "print(add(2, 3))"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["And now we create a decorator with arguments"]}, {"cell_type": "code", "execution_count": 31, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Decorator with arguments example:\n", "DEBUG: Calling multiply\n", "6\n"]}], "source": ["def log_call_with_prefix(prefix):\n", "    def decorator(func):\n", "        def wrapper(*args, **kwargs):\n", "            print(f\"{prefix}: Calling {func.__name__}\")\n", "            return func(*args, **kwargs)\n", "        return wrapper\n", "    return decorator\n", "\n", "@log_call_with_prefix(prefix=\"DEBUG\")\n", "def multiply(a, b):\n", "    return a * b\n", "\n", "print(\"Decorator with arguments example:\")\n", "print(multiply(2, 3))"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["It's a little bit annoying, because you have a function inside a function inside a function! But it's a very powerful tool for adding functionality to functions without modifying them."]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["And that's really all there is to it! The rest is just implementation details.\n", "\n", "But there is one more important thing to notice....what happens when we try to get the name of a wrapper?"]}, {"cell_type": "code", "execution_count": 33, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [{"data": {"text/plain": ["'wrapper'"]}, "execution_count": 33, "metadata": {}, "output_type": "execute_result"}], "source": ["multiply.__name__"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Well that's not good. What about the `help()` function?"]}, {"cell_type": "code", "execution_count": 35, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Help on function wrapper in module __main__:\n", "\n", "wrapper(*args, **kwargs)\n", "\n"]}], "source": ["help(multiply)"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["That's also not good! We seem to have lost all the information about the original function. This is a common problem with decorators, and there is a simple solution: the `functools.wraps` decorator."]}, {"cell_type": "code", "execution_count": 36, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from functools import wraps"]}, {"cell_type": "code", "execution_count": 37, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Decorator with arguments example:\n", "DEBUG: Calling multiply\n", "6\n"]}], "source": ["def log_call_with_prefix(prefix):\n", "    def decorator(func):\n", "        @wraps(func)\n", "        def wrapper(*args, **kwargs):\n", "            print(f\"{prefix}: Calling {func.__name__}\")\n", "            return func(*args, **kwargs)\n", "        return wrapper\n", "    return decorator\n", "\n", "@log_call_with_prefix(prefix=\"DEBUG\")\n", "def multiply(a, b):\n", "    return a * b\n", "\n", "print(\"Decorator with arguments example:\")\n", "print(multiply(2, 3))"]}, {"cell_type": "code", "execution_count": 38, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [{"data": {"text/plain": ["'multiply'"]}, "execution_count": 38, "metadata": {}, "output_type": "execute_result"}], "source": ["multiply.__name__"]}, {"cell_type": "code", "execution_count": 39, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Help on function multiply in module __main__:\n", "\n", "multiply(a, b)\n", "\n"]}], "source": ["help(multiply)"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Success! So `functools.wraps` is itself a decorator that preserves the information about the original function. Confused yet?"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Tracking latency\n", "Now we'll actually use this idea to track the latency of any function. We'll actually create a class."]}, {"cell_type": "code", "execution_count": 12, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from datetime import datetime\n", "import json\n", "from pathlib import Path\n", "import time\n", "import functools\n", "from typing import Any, Callable, Optional\n", "\n", "class LatencyLogger:\n", "    \"\"\"A decorator class for logging function execution times.\n", "    \"\"\"\n", "    def __init__(\n", "        self,\n", "        log: bool = True,\n", "        output_dir: str = './logs',\n", "        experiment_name: Optional[str] = None\n", "    ):\n", "        \"\"\"\n", "        Initialize the latency logger.\n", "        \n", "        Args:\n", "            log: Whether to enable logging (default: True)\n", "            output_dir: Directory where logs will be stored (default: './logs')\n", "            experiment_name: Optional name for this experiment\n", "        \"\"\"\n", "        self.log = log\n", "        \n", "        if self.log:\n", "            self.session_start = datetime.now()\n", "            self.experiment_name = experiment_name\n", "            \n", "            # Create session directory using date and UUID\n", "            date_str = self.session_start.strftime(\"%Y-%m-%d\")\n", "            self.session_dir = Path(output_dir) / self.experiment_name / date_str\n", "            self.session_dir.mkdir(parents=True, exist_ok=True)\n", "\n", "    def __call__(self, func: Callable) -> Callable:\n", "        \"\"\"\n", "        Decorator that wraps a function and logs its execution time.\n", "        \n", "        Args:\n", "            func: The function to be wrapped\n", "            \n", "        Returns:\n", "            Wrapped function that logs execution time\n", "        \"\"\"\n", "        @functools.wraps(func)\n", "        def wrapper(*args, **kwargs) -> Any:\n", "            start_time = time.time()\n", "            \n", "            try:\n", "                result = func(*args, **kwargs)\n", "                duration = time.time() - start_time\n", "                \n", "                if self.log:\n", "                    self._write_log('latency', {\n", "                        'timestamp': datetime.now().isoformat(),\n", "                        'function': func.__name__,\n", "                        'module': func.__module__,\n", "                        'duration_seconds': duration,\n", "                        'success': True,\n", "                        'args_count': len(args),\n", "                        'kwargs_count': len(kwargs)\n", "                    })\n", "                \n", "                return result\n", "                \n", "            except Exception as e:\n", "                duration = time.time() - start_time\n", "                \n", "                if self.log:\n", "                    self._write_log('latency', {\n", "                        'timestamp': datetime.now().isoformat(),\n", "                        'function': func.__name__,\n", "                        'module': func.__module__,\n", "                        'duration_seconds': duration,\n", "                        'success': False,\n", "                        'error_type': type(e).__name__,\n", "                        'error_message': str(e),\n", "                        'args_count': len(args),\n", "                        'kwargs_count': len(kwargs)\n", "                    })\n", "                raise\n", "                \n", "        return wrapper\n", "    \n", "    def _write_log(self, log_type: str, data: dict):\n", "        \"\"\"Write a log entry to the appropriate file.\"\"\"\n", "        file_path = self.session_dir / f\"{log_type}.jsonl\"\n", "        with open(file_path, 'a') as f:\n", "            json.dump(data, f)\n", "            f.write('\\n')"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Now, as before, we can instantiate out logger and use it to track the latency of any function."]}, {"cell_type": "code", "execution_count": 14, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["latency = LatencyLogger(\n", "    log=True,\n", "    output_dir='./logs',\n", "    experiment_name='log-test'\n", ")"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["And we can just wrap any function we want to track with the `@latency` decorator."]}, {"cell_type": "code", "execution_count": 15, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["@latency\n", "def get_response(user_input):\n", "    response = client.completion(\n", "        model='gpt-4o-mini',\n", "        messages=[\n", "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n", "            {\"role\": \"user\", \"content\": user_input},\n", "        ],\n", "        temperature=0.7,\n", "    )\n", "\n", "    return response.choices[0].message.content"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Let's try a couple of different prompts to see how long they take to generate."]}, {"cell_type": "code", "execution_count": 16, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["input_1 = \"Give me a one sentence summary of the movie Titanic.\"\n", "input_2 = \"Give me a detailed summary of the movie Titanic.\""]}, {"cell_type": "code", "execution_count": 17, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["response_1 = get_response(input_1)\n", "response_2 = get_response(input_2)"]}, {"cell_type": "code", "execution_count": 24, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["import json\n", "data = []\n", "with open('logs/log-test/2024-11-09/latency.jsonl', 'r') as f:\n", "    for line in f:\n", "        data.append(json.loads(line))"]}, {"cell_type": "code", "execution_count": 25, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from rich.pretty import pprint"]}, {"cell_type": "code", "execution_count": 26, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [{"data": {"text/html": ["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n", "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">\u2502   </span><span style=\"color: #008000; text-decoration-color: #008000\">'timestamp'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2024-11-06T19:45:54.978554'</span>,\n", "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">\u2502   </span><span style=\"color: #008000; text-decoration-color: #008000\">'function'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'get_response'</span>,\n", "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">\u2502   </span><span style=\"color: #008000; text-decoration-color: #008000\">'module'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'__main__'</span>,\n", "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">\u2502   </span><span style=\"color: #008000; text-decoration-color: #008000\">'duration_seconds'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.5884039402008057</span>,\n", "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">\u2502   </span><span style=\"color: #008000; text-decoration-color: #008000\">'success'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n", "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">\u2502   </span><span style=\"color: #008000; text-decoration-color: #008000\">'args_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n", "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">\u2502   </span><span style=\"color: #008000; text-decoration-color: #008000\">'kwargs_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n", "<span style=\"font-weight: bold\">}</span>\n", "</pre>\n"], "text/plain": ["\u001b[1m{\u001b[0m\n", "\u001b[2;32m\u2502   \u001b[0m\u001b[32m'timestamp'\u001b[0m: \u001b[32m'2024-11-06T19:45:54.978554'\u001b[0m,\n", "\u001b[2;32m\u2502   \u001b[0m\u001b[32m'function'\u001b[0m: \u001b[32m'get_response'\u001b[0m,\n", "\u001b[2;32m\u2502   \u001b[0m\u001b[32m'module'\u001b[0m: \u001b[32m'__main__'\u001b[0m,\n", "\u001b[2;32m\u2502   \u001b[0m\u001b[32m'duration_seconds'\u001b[0m: \u001b[1;36m1.5884039402008057\u001b[0m,\n", "\u001b[2;32m\u2502   \u001b[0m\u001b[32m'success'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n", "\u001b[2;32m\u2502   \u001b[0m\u001b[32m'args_count'\u001b[0m: \u001b[1;36m1\u001b[0m,\n", "\u001b[2;32m\u2502   \u001b[0m\u001b[32m'kwargs_count'\u001b[0m: \u001b[1;36m0\u001b[0m\n", "\u001b[1m}\u001b[0m\n"]}, "metadata": {}, "output_type": "display_data"}], "source": ["pprint(data[0])"]}, {"cell_type": "code", "execution_count": 27, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [{"data": {"text/html": ["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n", "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">\u2502   </span><span style=\"color: #008000; text-decoration-color: #008000\">'timestamp'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2024-11-06T19:46:04.549974'</span>,\n", "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">\u2502   </span><span style=\"color: #008000; text-decoration-color: #008000\">'function'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'get_response'</span>,\n", "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">\u2502   </span><span style=\"color: #008000; text-decoration-color: #008000\">'module'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'__main__'</span>,\n", "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">\u2502   </span><span style=\"color: #008000; text-decoration-color: #008000\">'duration_seconds'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9.57094407081604</span>,\n", "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">\u2502   </span><span style=\"color: #008000; text-decoration-color: #008000\">'success'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n", "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">\u2502   </span><span style=\"color: #008000; text-decoration-color: #008000\">'args_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n", "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">\u2502   </span><span style=\"color: #008000; text-decoration-color: #008000\">'kwargs_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n", "<span style=\"font-weight: bold\">}</span>\n", "</pre>\n"], "text/plain": ["\u001b[1m{\u001b[0m\n", "\u001b[2;32m\u2502   \u001b[0m\u001b[32m'timestamp'\u001b[0m: \u001b[32m'2024-11-06T19:46:04.549974'\u001b[0m,\n", "\u001b[2;32m\u2502   \u001b[0m\u001b[32m'function'\u001b[0m: \u001b[32m'get_response'\u001b[0m,\n", "\u001b[2;32m\u2502   \u001b[0m\u001b[32m'module'\u001b[0m: \u001b[32m'__main__'\u001b[0m,\n", "\u001b[2;32m\u2502   \u001b[0m\u001b[32m'duration_seconds'\u001b[0m: \u001b[1;36m9.57094407081604\u001b[0m,\n", "\u001b[2;32m\u2502   \u001b[0m\u001b[32m'success'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n", "\u001b[2;32m\u2502   \u001b[0m\u001b[32m'args_count'\u001b[0m: \u001b[1;36m1\u001b[0m,\n", "\u001b[2;32m\u2502   \u001b[0m\u001b[32m'kwargs_count'\u001b[0m: \u001b[1;36m0\u001b[0m\n", "\u001b[1m}\u001b[0m\n"]}, "metadata": {}, "output_type": "display_data"}], "source": ["pprint(data[1])"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["We can also compare this information with the runtime of the actual generation in the API call logs."]}, {"cell_type": "code", "execution_count": 28, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["import json\n", "token_usage = []\n", "with open('logs/log-test/2024-11-09/token_usage.jsonl', 'r') as f:\n", "    for line in f:\n", "        token_usage.append(json.loads(line))"]}, {"cell_type": "code", "execution_count": 29, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [{"data": {"text/plain": ["{'request_id': 'chatcmpl-AQgGRn1PbJxEgxhHp0b2UH0AOgMe5',\n", " 'timestamp': '2024-11-06T19:46:04.549635',\n", " 'model': 'gpt-4o-mini',\n", " 'prompt_tokens': 27,\n", " 'completion_tokens': 747,\n", " 'total_tokens': 774,\n", " 'duration_seconds': 9.570611000061035,\n", " 'costs': {'input': 4.05e-05, 'output': 0.004482, 'total': 0.0045225}}"]}, "execution_count": 29, "metadata": {}, "output_type": "execute_result"}], "source": ["token_usage[-1]"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Pretty much the same time."]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["What else can we do with this...?\n", "\n", "We will explore this later when it comes to metrics and evaluation of pipelines."]}], "metadata": {"kernelspec": {"display_name": "venv", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7"}}, "nbformat": 4, "nbformat_minor": 2}