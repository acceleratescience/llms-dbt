{"cells": [{"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Searching methods\n", "\n", "So far, we have covered _dense_ vector search over documents, but searching documents has been around for quite a long time. Google has been good at this for decades - long before the rise in transformer architectures.\n", "\n", "In this lesson we introduce **BM25**, a popular search algorithm, and one that you might hear being used in _hybrid search_ systems."]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## TF-IDF\n", "I just mentioned BM25, but before we get to that, let's talk about **TF-IDF**. This is a classic search algorithm that has been around for a long time. It stands for _Term Frequency-Inverse Document Frequency_.\n", "\n", "TF-IDF is a fundamental statistical measure used in information retrieval and text mining to evaluate how important a word is to a document within a collection (corpus). It combines two key components:\n", "\n", "- Term Frequency (TF): How often a word appears in a document\n", "- Inverse Document Frequency (IDF): How unique or rare that word is across all documents\n", "\n", "Before TF-IDF, systems often used simple word counts or frequencies, which had significant limitations:\n", "\n", "- Common words (like \"the\", \"is\", \"of\") would dominate the results\n", "- Rare but meaningful terms weren't given enough importance\n", "- Document length differences weren't properly accounted for"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Term Frequency (TF)\n", "There are several ways to calculate TF:\n", "\n", "Raw Frequency:\n", "```text\n", "tf(t,d) = number of times term t appears in document d\n", "```\n", "\n", "Boolean Frequency:\n", "```text\n", "tf(t,d) = 1 if t appears in d, 0 otherwise\n", "```\n", "\n", "Logarithmically Scaled Frequency:\n", "```text\n", "tf(t,d) = 1 + log(raw_frequency) if raw_frequency > 0, 0 otherwise\n", "```\n", "\n", "Normalized Frequency:\n", "```text\n", "tf(t,d) = raw_frequency / total_words_in_document\n", "```\n", "\n", "Inverse Document Frequency (IDF)\n", "The IDF is usually given as\n", "```text\n", "idf(t) = log(N / (1 + df(t)))\n", "```\n", "where:\n", "- `N` is the total number of documents in the corpus\n", "- `df(t)` is the number of documents containing term `t`\n", "\n", "You might see some different variations of the formula, but the idea is the same: the more documents that contain a term, the less unique or important it is.\n", "\n", "The final calculation for TF-IDF is simply the product of TF and IDF:\n", "```text\n", "tfidf(t,d) = tf(t,d) * idf(t)\n", "```"]}, {"cell_type": "code", "execution_count": 1, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["sentences = [\n", "    \"The quick brown fox jumps over the lazy dog\",\n", "    \"A bright sunny day in the park\",\n", "    \"Machine learning is a subset of artificial intelligence\",\n", "    \"Dogs and cats are common pets\",\n", "    \"The sun sets in the west\",\n", "    \"Artificial intelligence is changing the world\",\n", "    \"I love walking in the park on sunny days\",\n", "    \"The lazy cat sleeps all day\",\n", "    \"The trees in the park are beautiful\",\n", "]"]}, {"cell_type": "code", "execution_count": 2, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from sklearn.feature_extraction.text import TfidfVectorizer\n", "from sklearn.metrics.pairwise import cosine_similarity\n", "import numpy as np"]}, {"cell_type": "code", "execution_count": 3, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["vectorizer = TfidfVectorizer(stop_words='english')\n", "sentence_vectors = vectorizer.fit_transform(sentences)"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["What is this `sentence_vectors` variable?"]}, {"cell_type": "code", "execution_count": 4, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["feature_names = vectorizer.get_feature_names_out()\n", "dense_matrix = sentence_vectors.todense()"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["feature_names"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["These are the words that appear within the documents, minus any stop words."]}, {"cell_type": "code", "execution_count": 6, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["import pandas as pd"]}, {"cell_type": "code", "execution_count": 7, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["df = pd.DataFrame(\n", "    dense_matrix,\n", "    columns=feature_names,\n", "    index=[f\"Sentence {i+1}\" for i in range(len(sentences))]\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Print shape information\n", "print(\"Matrix Shape:\", sentence_vectors.shape)\n", "print(\"Number of non-zero elements:\", sentence_vectors.nnz)\n", "print(\"\\nVocabulary (feature names):\", len(feature_names))\n", "print(feature_names)\n", "\n", "# Show non-zero elements in first sentence\n", "print(\"\\nNon-zero elements in first sentence:\")\n", "first_sentence = df.iloc[0]\n", "print(first_sentence[first_sentence > 0].to_dict())"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["So each row of the matrix represents a document, and each column represents a word feature. The entry within each cell is the TF-IDF score for that word in that document.\n", "\n", "Each row represents a sentence\n", "```text\n", "Row 1 = \"The quick brown fox jumps over the lazy dog\"\n", "Row 2 = \"A bright sunny day in the park\"\n", "...etc\n", "```\n", "\n", "Each column represents a unique word (after removing stop words)\n", "```text\n", "Column 1 = 'artificial'\n", "Column 2 = 'bright'\n", "Column 3 = 'brown'\n", "...etc\n", "```\n", "\n", "For each word in each sentence\n", "```text\n", "0 = word doesn't appear\n", ">0 = TF-IDF score for that word in that sentence\n", "    - Higher if word appears frequently in sentence\n", "    - Higher if word is rare across all sentences\n", "```\n", "\n", "Here is an example of how a word score varies across some sentences:"]}, {"cell_type": "code", "execution_count": 9, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["\n", "TF-IDF scores for word 'bright' across sentences:\n", "Sentence 2: 0.581\n"]}], "source": ["word = 'bright'\n", "if word in feature_names:\n", "    print(f\"\\nTF-IDF scores for word '{word}' across sentences:\")\n", "    for i, score in enumerate(df[word]):\n", "        if score > 0:\n", "            print(f\"Sentence {i+1}: {score:.3f}\")"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Bright only appears in the second sentence, so it has a high score there."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["word = 'park'\n", "if word in feature_names:\n", "    print(f\"\\nTF-IDF scores for word '{word}' across sentences:\")\n", "    for i, score in enumerate(df[word]):\n", "        if score > 0:\n", "            print(f\"Sentence {i+1}: {score:.3f}\")"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Since \"park\" appears in many different sentences, it is not as unique, so it has a lower score.\n", "\n", "The TF-IDF score for \"park\" can differ between sentences even if it appears once in each because of how the sentences are structured. Here's why:\n", "\n", "1. Term Frequency (TF) part:\n", "\n", "    - If one sentence is shorter, the same single occurrence of \"park\" would have a higher TF score because TF is typically normalized by document length\n", "\n", "    - For example:\n", "\n", "        - \"Walk in the park\" (park is 1/4 of terms)\n", "        - \"I love walking in the beautiful park near my house\" (park is 1/9 of terms)\n", "\n", "\n", "2. Context words matter:\n", "\n", "    - If a sentence has more common words, the relative importance of \"park\" increases\n", "    \n", "    - If a sentence has more rare/unique words, the relative weight of each term is distributed differently\n", "\n", "\n", "3. ormalization:\n", "\n", "    - TF-IDF vectors are usually normalized (like in sklearn)\n", "\n", "    - So if a sentence contains other high-scoring terms, it will reduce the final score for \"park\" in that sentence due to the normalization\n", "\n", "    - For example, if \"park\" appears with very rare words, its relative contribution to the normalized vector will be smaller than if it appears with common words\n", "\n", "Even though \"park\" might appear once in multiple sentences, its final TF-IDF score can vary significantly based on sentence length, the other words present, and normalization effects."]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Searcing with TF-IDF\n", "OK, great, but how do we search with TF-IDF?"]}, {"cell_type": "code", "execution_count": 11, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["query = \"bright sunny park hippo\"\n", "n_results = 5\n", "# Transform the query into a vector\n", "query_vector = vectorizer.transform([query])\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["What is actually going on during the transformation process? For an example query as above, what is the output of the `transform` method?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["query_vector.todense()"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["We only have 3 words in the query that are in the vocabulary. The word 'hippo' is not in the vocabulary, so it is ignored."]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["If we look at the source code, we see that actually, we first apply the `CountVectorizer` to get the TF, then we simply use the IDF values that we found during the `fit` step."]}, {"cell_type": "code", "execution_count": 13, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["similarities = cosine_similarity(query_vector, sentence_vectors).flatten()\n", "top_indices = similarities.argsort()[::-1][:n_results]"]}, {"cell_type": "code", "execution_count": 14, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["results = []\n", "for idx in top_indices:\n", "    if similarities[idx] > 0:\n", "        results.append({\n", "            'score': round(similarities[idx], 3),\n", "            'text': sentences[idx]\n", "        })"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["results"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Notice that for the top result, the words \"bright\", \"sunny\" and \"park\" are all present in the query. This is why the score is so high.\n", "\n", "In contrast, the second result has \"bright\" and \"sunny\", but not \"park\", so the score is lower. And the final result has only the word \"park\", so the score is even lower."]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## A real world example\n", "OK, let's use this in anger.\n", "\n", "We have a dataset of scraped documentation from the Instructor python Library. We want to search this dataset for relevant documents based on a query.\n", "\n", "First we gather all of the above into a convenient class."]}, {"cell_type": "code", "execution_count": 16, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["import os\n", "from pathlib import Path\n", "from typing import List, Union\n", "from sklearn.feature_extraction.text import TfidfVectorizer\n", "from sklearn.metrics.pairwise import cosine_similarity\n", "from llama_index.core import SimpleDirectoryReader\n", "\n", "class TFIDFSearch:\n", "    def __init__(self):\n", "        self.vectorizer = TfidfVectorizer(stop_words='english')\n", "        self.doc_vectors = None\n", "        self.doc_paths = []\n", "    \n", "    \n", "    def read_file(self, file_path):\n", "        \"\"\"Read content from a file.\"\"\"\n", "        with open(file_path, 'r', encoding='utf-8') as file:\n", "            return file.read()\n", "\n", "    def load_documents(self, directory: Union[str, Path]) -> List[str]:\n", "        files = SimpleDirectoryReader(directory, recursive=True).load_data()\n", "        cwd = os.getcwd()\n", "        \n", "        # Create dictionary to store unique files\n", "        unique_files = {}\n", "        for doc in files:\n", "            path = \".\"+doc.metadata['file_path'][len(cwd):]\n", "            if path not in unique_files:\n", "                unique_files[path] = doc.text\n", "        \n", "        # Update class attributes\n", "        self.doc_paths = list(unique_files.keys())\n", "        return list(unique_files.values())\n", "\n", "    def fit_from_directory(self, directory: Union[str, Path]):\n", "        \"\"\"\n", "        Load documents from a directory and fit the TF-IDF model.\n", "        \n", "        Args:\n", "            directory: Path to directory containing text files\n", "        \"\"\"\n", "        # Load documents\n", "        documents = self.load_documents(directory)\n", "        \n", "        # Fit vectorizer and transform documents\n", "        self.doc_vectors = self.vectorizer.fit_transform(documents)\n", "        \n", "        print(f\"Processed {len(documents)} documents\")\n", "        print(f\"Vocabulary size: {len(self.vectorizer.get_feature_names_out())}\")\n", "\n", "    def search(self, query: str, n: int = 5) -> list[tuple]:\n", "        \"\"\"\n", "        Search for documents matching the query text.\n", "        \n", "        Args:\n", "            query: Raw query string\n", "            n: Number of documents to return\n", "            \n", "        Returns:\n", "            List of (score, file_path) tuples\n", "        \"\"\"\n", "        query_vector = self.vectorizer.transform([query])\n", "        similarities = cosine_similarity(query_vector, self.doc_vectors).flatten()\n", "        top_indices = similarities.argsort()[::-1][:n]\n", "        \n", "        results = []\n", "        for idx in top_indices:\n", "            if similarities[idx] > 0:  # Only include non-zero scores\n", "                text = self.read_file(self.doc_paths[idx])\n", "                results.append((similarities[idx], self.doc_paths[idx], text))\n", "        \n", "        return results\n", "\n", "    def get_feature_names(self):\n", "        \"\"\"Return the feature names (vocabulary).\"\"\"\n", "        return self.vectorizer.get_feature_names_out()\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["searcher = TFIDFSearch()\n", "\n", "searcher.fit_from_directory(\"data/test\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["query = \"bright sunny park hippo\"\n", "results = searcher.search(query)\n", "\n", "print(\"\\nSearch results:\")\n", "for score, path, text in results:\n", "    print(f\"Score: {score:.4f}, File: {path}, Text: {text}\")"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Great so this works as expected. We can search for documents based on a query. Now let's apply it to the Instructor documentation."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["searcher = TFIDFSearch()\n", "searcher.fit_from_directory(\"data/docs\")"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["There are a bunch of errors. Don't worry about those, it's because we're trying to parse images. We'll fix that later."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["query = \"I am trying to extract some data from a receipt using OpenAI models. Can you help me with that?\"\n", "results = searcher.search(query, n=10)\n", "\n", "print(\"\\nSearch results:\\n\")\n", "for score, path, text in results:\n", "    print(f\"Score: {score:.4f}, File: {path}\")"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Well, that top one looks promising! What does it say?"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Extracting Receipt Data using GPT-4 and Python\n", "\n", "This post demonstrates how to use Python's Pydantic library and OpenAI's GPT-4 model to extract receipt data from images and validate the total amount. This method is particularly useful for automating expense tracking and financial analysis tasks.\n", "\n", "## Defining the Item and Receipt Classes\n", "\n", "First, we define two Pydantic models, `Item` and `Receipt`, to structure the extracted data. The `Item` class represents individual items on the receipt, with fields for name, price, and quantity. The `Receipt` class contains a list of `Item` objects and the total amount.\n", "\n", "```python\n", "from pydantic import BaseModel\n", "\n", "\n", "class Item(BaseModel):\n", "    name: str\n", "    price: float\n", "    quantity: int\n", "\n", "\n", "class Receipt(BaseModel):\n", "    items: list[Item]\n", "    total: float\n", "```\n", "\n", "## Validating the Total Amount\n", "\n", "To ensure the accuracy of the extracted data, we use Pydantic's `model_validator` decorator to define a custom validation function, `check_total`. This function calculates the sum of item prices and compares it to the extracted total amount. If there's a discrepancy, it raises a `ValueError`.\n", "\n", "```python\n", "from pydantic import model_validator\n", "\n", "\n", "@model_validator(mode=\"after\")\n", "def check_total(self):\n", "    items = self.items\n", "    total = self.total\n", "    calculated_total = sum(item.price * item.quantity for item in items)\n", "    if calculated_total != total:\n", "        raise ValueError(\n", "            f\"Total {total} does not match the sum of item prices {calculated_total}\"\n", "        )\n", "    return self\n", "```\n", "\n", "## Extracting Receipt Data from Images\n", "\n", "The `extract_receipt` function uses OpenAI's GPT-4 model to process an image URL and extract receipt data. We utilize the `instructor` library to configure the OpenAI client for this purpose.\n", "\n", "```python\n", "import instructor\n", "from openai import OpenAI\n", "\n", "# <%hide%>\n", "from pydantic import BaseModel, model_validator\n", "\n", "\n", "class Item(BaseModel):\n", "    name: str\n", "    price: float\n", "    quantity: int\n", "\n", "\n", "class Receipt(BaseModel):\n", "    items: list[Item]\n", "    total: float\n", "\n", "    @model_validator(mode=\"after\")\n", "    def check_total(cls, values: \"Receipt\"):\n", "        items = values.items\n", "        total = values.total\n", "        calculated_total = sum(item.price * item.quantity for item in items)\n", "        if calculated_total != total:\n", "            raise ValueError(\n", "                f\"Total {total} does not match the sum of item prices {calculated_total}\"\n", "            )\n", "        return values\n", "\n", "\n", "# <%hide%>\n", "\n", "client = instructor.from_openai(OpenAI())\n", "\n", "\n", "def extract(url: str) -> Receipt:\n", "    return client.chat.completions.create(\n", "        model=\"gpt-4\",\n", "        max_tokens=4000,\n", "        response_model=Receipt,\n", "        messages=[\n", "            {\n", "                \"role\": \"user\",\n", "                \"content\": [\n", "                    {\n", "                        \"type\": \"image_url\",\n", "                        \"image_url\": {\"url\": url},\n", "                    },\n", "                    {\n", "                        \"type\": \"text\",\n", "                        \"text\": \"Analyze the image and return the items in the receipt and the total amount.\",\n", "                    },\n", "                ],\n", "            }\n", "        ],\n", "    )\n", "```\n", "\n", "## Practical Examples\n", "\n", "In these examples, we apply the method to extract receipt data from two different images. The custom validation function ensures that the extracted total amount matches the sum of item prices.\n", "\n", "```python\n", "# <%hide%>\n", "from pydantic import BaseModel, model_validator\n", "import instructor\n", "from openai import OpenAI\n", "\n", "\n", "class Item(BaseModel):\n", "    name: str\n", "    price: float\n", "    quantity: int\n", "\n", "\n", "class Receipt(BaseModel):\n", "    items: list[Item]\n", "    total: float\n", "\n", "    @model_validator(mode=\"after\")\n", "    def check_total(cls, values: \"Receipt\"):\n", "        items = values.items\n", "        total = values.total\n", "        calculated_total = round(sum(item.price * item.quantity for item in items), 2)\n", "        if calculated_total != total:\n", "            raise ValueError(\n", "                f\"Total {total} does not match the sum of item prices {calculated_total}\"\n", "            )\n", "        return values\n", "\n", "\n", "client = instructor.from_openai(OpenAI())\n", "\n", "\n", "def extract(url: str) -> Receipt:\n", "    return client.chat.completions.create(\n", "        model=\"gpt-4o\",\n", "        max_tokens=4000,\n", "        response_model=Receipt,\n", "        messages=[\n", "            {\n", "                \"role\": \"user\",\n", "                \"content\": [\n", "                    {\n", "                        \"type\": \"image_url\",\n", "                        \"image_url\": {\"url\": url},\n", "                    },\n", "                    {\n", "                        \"type\": \"text\",\n", "                        \"text\": \"Analyze the image and return the items in the receipt and the total amount.\",\n", "                    },\n", "                ],\n", "            }\n", "        ],\n", "    )\n", "\n", "\n", "# <%hide%>\n", "url = \"https://templates.mediamodifier.com/645124ff36ed2f5227cbf871/supermarket-receipt-template.jpg\"\n", "\n", "\n", "receipt = extract(url)\n", "print(receipt)\n", "```\n", "\n", "By combining the power of GPT-4 and Python's Pydantic library, we can accurately extract and validate receipt data from images, streamlining expense tracking and financial analysis tasks.\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Let's try another example:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["query = \"How do I use Instructor with OpenAI models?\"\n", "results = searcher.search(query, n=10)\n", "\n", "print(\"\\nSearch results:\\n\")\n", "for score, path, text in results:\n", "    print(f\"Score: {score:.4f}, File: {path}\")"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Well this is not so good. None of these scores are particularly useful to me..."]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## BM25\n", "There are a number of ways to improve on basic TF-IDF search. One popular method is **BM25**.\n", "\n", "#### Term Saturation\n", "If the word cat appears 10 times in a document, it's probably not 10 times more important than if it appeared once. BM25 accounts for this by using a saturation function.\n", "\n", "```text\n", "term_score = tf / (tf + k)\n", "```\n", "\n", "In addition, we also care about the document length. If a document contains the word \"cat\" only once and it is short, then the word \"cat\" is still likely to be relevant. However, if the document is very long, then the word \"cat\" is less likely to be relevant.\n", "\n", "As the k parameter increases, the value of TF/(TF + k) decreases. To penalize long documents, we can increase k if the document is longer than average, and decrease k if the document is shorter than average. We\u2019ll achieve this by multiplying k by the ratio dl/adl. Here, dl is the document\u2019s length, and adl is the average document length across the corpus.\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["We're not going to implement BM25 from scratch, so we use a library instead."]}, {"cell_type": "code", "execution_count": 26, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["import os\n", "from pathlib import Path\n", "from typing import List, Union\n", "import bm25s\n", "from llama_index.core import SimpleDirectoryReader\n", "\n", "class BM25Search:\n", "    def __init__(self):\n", "        self.retriever = None\n", "        self.doc_paths = []\n", "        self.documents = []\n", "    \n", "    def read_file(self, file_path):\n", "        \"\"\"Read content from a file.\"\"\"\n", "        with open(file_path, 'r', encoding='utf-8') as file:\n", "            return file.read()\n", "\n", "    def load_documents(self, directory: Union[str, Path]) -> List[str]:\n", "        files = SimpleDirectoryReader(directory, recursive=True).load_data()\n", "        cwd = os.getcwd()\n", "        \n", "        # Create dictionary to store unique files\n", "        unique_files = {}\n", "        for doc in files:\n", "            path = \".\"+doc.metadata['file_path'][len(cwd):]\n", "            if path not in unique_files:\n", "                unique_files[path] = doc.text\n", "        \n", "        # Update class attributes\n", "        self.doc_paths = list(unique_files.keys())\n", "        return list(unique_files.values())\n", "\n", "    def fit_from_directory(self, directory: Union[str, Path], **kwargs):\n", "        \"\"\"\n", "        Load documents from a directory and create the BM25 index.\n", "        \n", "        Args:\n", "            directory: Path to directory containing text files\n", "        \"\"\"\n", "        # Load documents\n", "        self.documents = self.load_documents(directory)\n", "        \n", "        # Tokenize documents and create index\n", "        corpus_tokens = bm25s.tokenize(self.documents, stopwords=\"en\")\n", "        self.retriever = bm25s.BM25(**kwargs)\n", "        self.retriever.index(corpus_tokens)\n", "        \n", "        print(f\"Processed {len(self.documents)} documents\")\n", "\n", "    def search(self, query: str, n: int = 5) -> list[tuple]:\n", "        \"\"\"\n", "        Search for documents matching the query text.\n", "        \n", "        Args:\n", "            query: Raw query string\n", "            n: Number of documents to return\n", "            \n", "        Returns:\n", "            List of (score, file_path, text) tuples\n", "        \"\"\"\n", "        query_tokens = bm25s.tokenize(query, stopwords=\"en\")\n", "        docs, scores = self.retriever.retrieve(query_tokens, corpus=self.documents, k=n)\n", "        \n", "        results = []\n", "        # Make sure we're accessing the first element of docs and scores\n", "        for doc, score in zip(docs[0], scores[0]):\n", "            if score > 0:  # Only include non-zero scores\n", "                doc_idx = self.documents.index(doc)\n", "                text = self.read_file(self.doc_paths[doc_idx])\n", "                results.append((score, self.doc_paths[doc_idx], text))\n", "        \n", "        return sorted(results, key=lambda x: x[0], reverse=True)\n", "\n", "    def save_index(self, path: str):\n", "        \"\"\"Save the BM25 index to disk.\"\"\"\n", "        self.retriever.save(path)\n", "    \n", "    def load_index(self, path: str):\n", "        \"\"\"Load a previously saved BM25 index.\"\"\"\n", "        self.retriever = bm25s.BM25.load(path, load_corpus=True)"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Not much else changes. We can test it out with the test sentences."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["searcher = BM25Search()\n", "searcher.fit_from_directory(\"data/test\")\n", "\n", "results = searcher.search(\"bright sunny park hippo\", n=3)\n", "print(\"\\nSearch results:\")\n", "for score, path, text in results:\n", "    print(f\"Score: {score:.4f}, File: {path}, Text: {text}\")"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Since that works, let's try with the Instructor documentation."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["searcher = BM25Search()\n", "searcher.fit_from_directory(\"data/docs\")\n", "\n", "results = searcher.search(\"I am trying to extract some data from a receipt using OpenAI models.\", n=10)\n", "print(\"\\nSearch results:\")\n", "for score, path, text in results:\n", "    print(f\"Score: {score:.4f}, File: {path}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["results = searcher.search(\"How do I use Instructor with OpenAI models?\", n=10)\n", "print(\"\\nSearch results:\")\n", "for score, path, text in results:\n", "    print(f\"Score: {score:.4f}, File: {path}\")"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Well, at least these aren't just blog posts... but still, not very useful. Things to play with include the `k1` and `b` parameters."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["searcher = BM25Search()\n", "searcher.fit_from_directory(\"data/docs\", k1=0.5,\n", "        b=1.2,\n", "        delta=0.1)\n", "\n", "results = searcher.search(\"How do I use Instructor with OpenAI models?\", n=10)\n", "print(\"\\nSearch results:\")\n", "for score, path, text in results:\n", "    print(f\"Score: {score:.4f}, File: {path}\")"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["In the next section, we look at how well dense vector searches will do."]}], "metadata": {"kernelspec": {"display_name": "venv", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7"}}, "nbformat": 4, "nbformat_minor": 2}