{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# RAG\n",
    "Remember this image?\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"./img/pipeline.png\" alt=\"drawing\" style=\"width:800px;\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's implement it using the methods we have learnt so far.\n",
    "\n",
    "We will build a very basic RAG system over the Instructor documentation. Previously, we tried to use BM25, so let's now have a look at using a ChromaDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "api_key = \"abcd1234\"\n",
    "\n",
    "client = OpenAI(api_key = api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's first find out if `gpt-4o-mini` can help us extract reciept information using the Instructor library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def generate(text: str, **kwargs) -> str:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": text},\n",
    "        ],\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    return response\n",
    "\n",
    "response = generate(\"I am trying to extract some data from a picture of a receipt using the Instructor library. Can you help me with that?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Obviously this is not using the Instructor library. Does `gpt-4o-mini` even know about Instructor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "response = generate(\"Are you aware of the python Instructor library?\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "OK, so no it doesn't...In the `data/docs` folder there is the documentation of the Instructor library. We will use this documentation to build a RAG system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We are going to follow a similar pattern to the `BM25Search` class. In the class definition below, fill in the missing components.\n",
    "\n",
    "As a reminder, the `BM25Search` class had the following methods:\n",
    "\n",
    "```python\n",
    "class BM25Search\n",
    "\n",
    "    def __init__()\n",
    "\n",
    "    def read_file()\n",
    "\n",
    "    def load_documents()\n",
    "\n",
    "    def fit_from_directory()\n",
    "\n",
    "    def search()\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "class ChromaSearch\n",
    "    # Same named methods as BM25Search\n",
    "    def __init__()\n",
    "\n",
    "    def read_file()\n",
    "\n",
    "    def load_documents()\n",
    "\n",
    "    def fit_from_directory()\n",
    "\n",
    "    def search()\n",
    "\n",
    "    # New methods\n",
    "    def get_embedding()\n",
    "\n",
    "    def embedding_fn()\n",
    "\n",
    "    def clear_index()\n",
    "\n",
    "    def format_context()\n",
    "\n",
    "    def search_and_format()\n",
    "\n",
    "    def clear_index()\n",
    "```\n",
    "\n",
    "So there is a little bit of work to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the `utils.py` file, we have the `TemplateManager` class which we have already met, but we have also defined a `BaseSearcher` class:\n",
    "\n",
    "```python\n",
    "class BaseSearcher(ABC):\n",
    "    \"\"\"Abstract base class defining the interface for search implementations.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.doc_paths = []\n",
    "        self.documents = []\n",
    "    \n",
    "    @abstractmethod\n",
    "    def fit_from_directory(self, directory: Union[str, Path], **kwargs):\n",
    "        \"\"\"Load and index documents from a directory.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def search(self, query: str, n: int = 5) -> List[Tuple[float, str, str]]:\n",
    "        \"\"\"Search for documents matching the query.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def read_file(self, file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "\n",
    "    def load_documents(self, directory: Union[str, Path]) -> List[str]:\n",
    "        files = SimpleDirectoryReader(directory, recursive=True).load_data()\n",
    "        cwd = os.getcwd()\n",
    "        \n",
    "        unique_files = {}\n",
    "        for doc in files:\n",
    "            path = \".\"+doc.metadata['file_path'][len(cwd):]\n",
    "            if path not in unique_files:\n",
    "                unique_files[path] = doc.text\n",
    "        \n",
    "        self.doc_paths = list(unique_files.keys())\n",
    "        return list(unique_files.values())\n",
    "    \n",
    "    def format_context(self, results: List[Tuple[float, str, str]]) -> str:\n",
    "        \"\"\"Format search results into a structured context string.\"\"\"\n",
    "        formatted_text = \"\"\n",
    "        for score, file_path, content in results:\n",
    "            # Extract title from file path (remove extension and path)\n",
    "            title = os.path.splitext(os.path.basename(file_path))[0]\n",
    "            \n",
    "            formatted_text += f\"title: {title}\\n\"\n",
    "            formatted_text += f\"similarity score: {score:.4f}\\n\"\n",
    "            formatted_text += f\"content: {content}\\n\\n\"\n",
    "        \n",
    "        return formatted_text.strip()\n",
    "    \n",
    "    def search_and_format(self, query: str, n: int = 5) -> str:\n",
    "        \"\"\"Search and format results in one call.\"\"\"\n",
    "        results = self.search(query, n)\n",
    "        return self.format_context(results)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The BM25 and the Chroma searcher will share some common methods. In addition, we have put in the requirement that the searcher must always implement the `fit_from_directory` and `search` methods.\n",
    "\n",
    "In `utils` we have also refactored the BM25Search class to inherit from `BaseSearcher`. We will now construct a new class `ChromaSearch` that will inherit from `BaseSearcher`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from utils import BaseSearcher, BM25Search\n",
    "\n",
    "import chromadb\n",
    "\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So we **must** implement the `fit_from_directory` and `search` methods in the `ChromaSearch` class. But we should also implement a way to get the embeddings using the OpenAI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class ChromaSearch(BaseSearcher):\n",
    "    def __init__(self, collection_name: str = \"documents\", model_name: str = \"text-embedding-3-small\"):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        # Initialize the database client\n",
    "        self.db_client = chromadb.PersistentClient(path=f\"./databases/{collection_name}/\")\n",
    "\n",
    "        # Get or create the collection\n",
    "        self.collection = self.db_client.get_or_create_collection(\n",
    "            name=collection_name,\n",
    "            metadata={\"hnsw:space\": \"cosine\"}\n",
    "        )\n",
    "\n",
    "        self.openai = OpenAI(api_key = api_key)\n",
    "\n",
    "\n",
    "    def get_embedding(self, text: str | list) -> dict:\n",
    "        \"\"\"Get embedding from OpenAI API.\"\"\"\n",
    "        # Ensure text is in the correct format\n",
    "        if isinstance(text, list):\n",
    "            input_text = text\n",
    "        else:\n",
    "            input_text = [text]\n",
    "        \n",
    "        # Get the embeddings\n",
    "        response = self.openai.embeddings.create(\n",
    "            model=self.model_name,\n",
    "            input=input_text\n",
    "        )\n",
    "        return response\n",
    "\n",
    "\n",
    "    def fit_from_directory(self, directory: str | Path):\n",
    "        self.documents = self.load_documents(directory)\n",
    "        # We will add items in batches of 100\n",
    "        for i in range(0, len(self.documents), 100):\n",
    "            batch_docs = self.documents[i:i+100]\n",
    "            batch_paths = self.doc_paths[i:i+100]\n",
    "            \n",
    "            embeddings = self.get_embedding(batch_docs)\n",
    "            embeddings = [embedding.embedding for embedding in embeddings.data]\n",
    "            batch_ids = [str(j) for j in range(i, i + len(batch_docs))]\n",
    "\n",
    "            # Add the embeddings to the collection\n",
    "            self.collection.add(\n",
    "                embeddings=embeddings,\n",
    "                documents=batch_docs,\n",
    "                metadatas=[{\"file_path\": path} for path in batch_paths],\n",
    "                ids=batch_ids\n",
    "            )\n",
    "        \n",
    "        print(f\"Processed {len(self.documents)} documents\")\n",
    "\n",
    "\n",
    "    def search(self, query: str, n: int = 5) -> list[tuple[float, str, str]]:\n",
    "        query_embedding = self.get_embedding(query).data[0].embedding\n",
    "\n",
    "        # Search the collection\n",
    "        results = self.collection.query(\n",
    "            query_embeddings=[query_embedding],\n",
    "            n_results=n,\n",
    "            include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "        )\n",
    "        \n",
    "        # Some formatting\n",
    "        formatted_results = []\n",
    "        for doc, metadata, distance in zip(\n",
    "            results['documents'][0],\n",
    "            results['metadatas'][0],\n",
    "            results['distances'][0]\n",
    "        ):\n",
    "            score = 1 - distance\n",
    "            file_path = metadata['file_path']\n",
    "            text = self.read_file(file_path)\n",
    "            formatted_results.append((score, file_path, text))\n",
    "            \n",
    "        return sorted(formatted_results, key=lambda x: x[0], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the search\n",
    "searcher = ChromaSearch(collection_name=\"instructor-docs\", model_name=\"text-embedding-3-small\")\n",
    "searcher.fit_from_directory(\"data/docs\")\n",
    "\n",
    "bm25_searcher = BM25Search()\n",
    "bm25_searcher.fit_from_directory(\"data/docs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's compare which links each searcher returns for a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "query = \"I am trying to extract some data from a picture of a receipt using the Instructor library. Can you help me with that?\"\n",
    "\n",
    "chroma_results = searcher.search(query)\n",
    "bm25_results = bm25_searcher.search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "chroma_links = [result[1] for result in chroma_results]\n",
    "bm25_links = [result[1] for result in bm25_results]\n",
    "\n",
    "print(\"Chroma Results:\")\n",
    "for link in chroma_links:\n",
    "    print(link)\n",
    "\n",
    "print(\"\\nBM25 Results:\")\n",
    "for link in bm25_links:\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Chat integration\n",
    "We want to be able to ask questions about our documentation. We have two promps:\n",
    "\n",
    "#### System Prompt\n",
    "---\n",
    "```jinja\n",
    "You are an expert on the Instructor library. Instructor makes it easy to get structured data like JSON from LLMs like GPT-3.5, GPT-4, GPT-4-Vision, and open-source models including Mistral/Mixtral, Anyscale, Ollama, and llama-cpp-python.\n",
    "\n",
    "You will be asked queries from a user. The query from the user will be used to query a database of Instructor documentation in markdown format. You will be provided with the top N search results from this database. Your task is to provide a response to the user based on the search results.\n",
    "```\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "#### User Prompt\n",
    "---\n",
    "```jinja\n",
    "Here are the search results from the database::\n",
    "\n",
    "## Search Results ##\n",
    "{{ results }}\n",
    "\n",
    "Here is the user query:\n",
    "\n",
    "## User Query ##\n",
    "{{ query }}\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from utils import TemplateManager\n",
    "\n",
    "# Initialize the template manager\n",
    "# template_manager = ...\n",
    "template_manager = TemplateManager('./prompts')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def get_links(results: list[tuple[float, str, str]]) -> str:\n",
    "    links = \"\\n\\nHere are some links that might be useful:\\n\\n\"\n",
    "    base = \"https://python.useinstructor.com\"\n",
    "    for result in results:\n",
    "        # get the link name by removing the \"docs\" prefix and removing .md extension\n",
    "        path = result[1].replace(\"./data/docs\", \"\")\n",
    "        path = path.replace(\".md\", \"\")\n",
    "        links += f\" - {base}{path}\\n\"\n",
    "\n",
    "    return links\n",
    "\n",
    "\n",
    "def generate_response(query: str, searcher: BaseSearcher, model: str = \"gpt-4o-mini\", n=3) -> str:\n",
    "    # First get the search results\n",
    "    results = searcher.search(query, n=n)\n",
    "    \n",
    "    # Format the results using format_context\n",
    "    formatted_results = searcher.format_context(results)\n",
    "\n",
    "    # Get the system prompt and user prompt\n",
    "    system_prompt = template_manager.render('system_prompt.jinja')\n",
    "    user_input = template_manager.render('user_prompt.jinja', results=formatted_results, query=query)\n",
    "\n",
    "    # Generate the response\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_input},\n",
    "        ]\n",
    "    ).choices[0].message.content\n",
    "\n",
    "    links = get_links(results)\n",
    "    response = f\"{response}\\n\\n{links}\"\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "query = \"Can you give me an example of how to use the Instructor library with Anthropic models?\"\n",
    "\n",
    "chroma_response = generate_response(query, searcher)\n",
    "bm25_response = generate_response(query, bm25_searcher)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Chroma Response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Certainly! Here’s an example of how to integrate the Instructor library with Anthropic models to create structured outputs using Pydantic models. This example demonstrates how to set up the client and generate a user model with specific properties.\n",
    "\n",
    "### Step 1: Install Required Libraries\n",
    "\n",
    "First, you need to install the necessary libraries. You can do this via pip:\n",
    "\n",
    "```bash\n",
    "pip install anthropic instructor[anthropic]\n",
    "```\n",
    "\n",
    "### Step 2: Set Up Your Python Script\n",
    "\n",
    "Here’s a complete example:\n",
    "\n",
    "```python\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "import anthropic\n",
    "import instructor\n",
    "\n",
    "# Patch the Anthropic client with the Instructor for enhanced capabilities\n",
    "anthropic_client = instructor.from_anthropic(\n",
    "    create=anthropic.Anthropic()\n",
    ")\n",
    "\n",
    "# Define your Pydantic models\n",
    "class Properties(BaseModel):\n",
    "    name: str\n",
    "    value: str\n",
    "\n",
    "class User(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "    properties: List[Properties]\n",
    "\n",
    "# Use the patched client to generate structured output\n",
    "user_response = anthropic_client(\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    max_tokens=1024,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Create a user for a model with a name, age, and properties.\",\n",
    "        }\n",
    "    ],\n",
    "    response_model=User,\n",
    ")\n",
    "\n",
    "print(user_response.model_dump_json(indent=2))\n",
    "```\n",
    "\n",
    "### Explanation of the Code\n",
    "\n",
    "1. **Imports**: The script imports necessary classes and functions from Pydantic, along with the Anthropics and Instructor libraries.\n",
    "2. **Client Setup**: It patches the Anthropic client with the Instructor client, enabling enhanced capabilities for generating structured outputs.\n",
    "3. **Model Definitions**: Pydantic models `Properties` and `User` are defined to structure the content received from the model.\n",
    "4. **Make a Request**: A request is made to the Anthropic model (in this case, “claude-3-haiku-20240307”) to create a user with specified attributes.\n",
    "5. **Output**: The response is printed in a structured JSON format.\n",
    "\n",
    "### Sample Output\n",
    "\n",
    "The execution of this script would yield a structured output similar to the following:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"name\": \"John Doe\",\n",
    "  \"age\": 35,\n",
    "  \"properties\": [\n",
    "    {\n",
    "      \"name\": \"City\",\n",
    "      \"value\": \"New York\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"Occupation\",\n",
    "      \"value\": \"Software Engineer\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "This approach allows you to efficiently extract structured data from the Anthropic model's responses using the Instructor library. Happy coding!\n",
    "\n",
    "\n",
    "\n",
    "Here are some links that might be useful:\n",
    "\n",
    " - https://python.useinstructor.com/hub/anthropic\n",
    " - https://python.useinstructor.com/blog/posts/anthropic\n",
    " - https://python.useinstructor.com/blog/posts/structured-output-anthropic\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# BM25 Response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Certainly! You can use the Instructor library with Anthropic models to create user models with complex properties. Here’s an example that demonstrates how to integrate the Anthropic client with the Instructor client in Python:\n",
    "\n",
    "### Installation\n",
    "Make sure you have the Anthropic library installed:\n",
    "```bash\n",
    "pip install anthropic\n",
    "```\n",
    "\n",
    "### Example Code\n",
    "Here's how you can set up the Instructor client with an Anthropic model:\n",
    "\n",
    "```python\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "import anthropic\n",
    "import instructor\n",
    "\n",
    "# Patching the Anthropic client with the instructor for enhanced capabilities\n",
    "client = instructor.from_anthropic(\n",
    "    anthropic.Anthropic(),\n",
    ")\n",
    "\n",
    "# Define your data models\n",
    "class Properties(BaseModel):\n",
    "    name: str\n",
    "    value: str\n",
    "\n",
    "class User(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "    properties: List[Properties]\n",
    "\n",
    "# Make a request to create a user with specific attributes\n",
    "user_response = client.chat.completions.create(\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    max_tokens=1024,\n",
    "    max_retries=0,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Create a user for a model with a name, age, and properties.\",\n",
    "        }\n",
    "    ],\n",
    "    response_model=User,\n",
    ")\n",
    "\n",
    "# Print the resulting user model in JSON format\n",
    "print(user_response.model_dump_json(indent=2))\n",
    "```\n",
    "\n",
    "### Expected Output\n",
    "When you run this code, you might get a response similar to the following (the actual output will depend on the model's response):\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"name\": \"John Doe\",\n",
    "  \"age\": 35,\n",
    "  \"properties\": [\n",
    "    {\n",
    "      \"name\": \"City\",\n",
    "      \"value\": \"New York\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"Occupation\",\n",
    "      \"value\": \"Software Engineer\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "### Explanation\n",
    "- **Properties Class**: This defines a structure for properties that can be linked to a user, such as name and value pairs.\n",
    "- **User Class**: This encapsulates the user's details, including their basic information and a list of properties.\n",
    "- **Request to the Anthropic Model**: You send a request to the Anthropic model (replace `\"claude-3-haiku-20240307\"` with the model you wish to use), and specify that you want a response conforming to the `User` model structure.\n",
    "- **Response Handling**: The `model_dump_json` method allows you to output the structured response as formatted JSON.\n",
    "\n",
    "This integration enhances the capabilities of your application by allowing you to define detailed user profiles while leveraging the powerful language generation capabilities of Anthropic models.\n",
    "\n",
    "\n",
    "\n",
    "Here are some links that might be useful:\n",
    "\n",
    " - https://python.useinstructor.com/prompting/zero_shot/role_prompting\n",
    " - https://python.useinstructor.com/prompting/thought_generation/chain_of_thought_few_shot/auto_cot\n",
    " - https://python.useinstructor.com/hub/anthropic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
