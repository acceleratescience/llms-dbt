{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The OpenAI API\n",
    "\n",
    "In this section we will cover the basics of using the OpenAI API, including:\n",
    "- Chat Completions\n",
    "- Streaming\n",
    "- Vision input\n",
    "\n",
    "The beauty of the OpenAI API is that is very simple to use.\n",
    "\n",
    "In your environment you should have a file called `.env` with the following:\n",
    "\n",
    "```bash\n",
    "OPENAI_API_KEY=\"sk-proj-1234567890\"\n",
    "```\n",
    "\n",
    "We will give you this key in the workshop. __The key will be deactivated after the workshop!__\n",
    "\n",
    "You can then grab the key using python:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import dotenv\n",
    "import os\n",
    "from rich import print as rprint # for making fancy outputs\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Completions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling a model is simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silent moonlit nights,  \n",
      "A shadow glides through the weedsâ€”  \n",
      "Fierce heart, whiskers poised.\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"You are Matsuo Basho, the great Japanese haiku poet.\"\n",
    "user_query = \"Can you give me a haiku about a Samurai cat.\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_query},\n",
    "  ],\n",
    "  max_tokens=128\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purrfect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The API offers a number of _endpoints_ that allow you to interact with the models. The main one that we will cover here is the `/chat/completions` endpoint. This endpoint allows you to interact with the model in a conversational manner.\n",
    "\n",
    "Only 2 arguments are actually required for this endpoint:\n",
    "\n",
    "- `model: str` The model to use. For OpenAI, this includes:\n",
    "    - `'gpt-3.5-turbo'`\n",
    "\n",
    "    - `'gpt-4'`\n",
    "\n",
    "    - `'gpt-4o'`\n",
    "\n",
    "    - `'gpt-4o-mini'`\n",
    "    \n",
    "    - Any fine-tuned versions of these models.\n",
    "    \n",
    "    - Many specific versions of the above models.\n",
    "\n",
    "- `messages: list` A list of messages that the model should use to generate a response. Each entry in the list of messages comes in the form:\n",
    "\n",
    "```python\n",
    "{\"role\": \"<role>\", \"content\": \"<content>\", \"name\": \"<name>\"}\n",
    "```\n",
    "\n",
    "Where `<role>` can take one of the following forms:\n",
    "\n",
    "- `'system'` This is a system level prompt, designed to guide the conversation. For example: \n",
    "\n",
    "_\"You are a customer service bot.\"_\n",
    "\n",
    "- `'user'` This is direct input from the user. For example: \n",
    "\n",
    "_\"How do I reset my password?\"_\n",
    "\n",
    "- `'assistant'` This is the response from the model. For example:\n",
    "\n",
    "_\"To reset your password, please visit our website and click on the 'Forgot Password' link.\"_\n",
    "\n",
    "So all of this fed into one message list would look like this:\n",
    "\n",
    "```python\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a customer service bot.\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I reset my password?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"To reset your password, please visit our website and click on the 'Forgot Password' link.\"}\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional arguments\n",
    "The `/chat/completions` endpoint also accepts a number of additional arguments that can be used to alter the response. These include (arguments are listed with their default values if applicable):\n",
    "\n",
    "- `max_tokens: int` The maximum number of tokens to generate in the response. Important to stop the model from generating too much text and racking up a huge bill.\n",
    "\n",
    "- `n: int = 1` The number of completions to generate. This is useful when you want to generate multiple completions and select the best one. You'll be charged for the _**total**_ number of tokens generated across all completions, so be careful with setting this too high.\n",
    "\n",
    "- `temperature: float = 1.0` The temperature of the model, ranging from 0.0 to 2. Use low values for deterministic responses, and high values for more creative responses.\n",
    "\n",
    "- `top_p: float = 1.0` The probability of sampling from the top `p` tokens. This is useful for controlling the diversity of the responses. Setting this to a higher values means the model is more likely to sample from a wider range of tokens.\n",
    "\n",
    "- `logprobs: bool = False` Whether to return the log probabilities of the tokens generated. This is useful when you want to understand how the model is making decisions.\n",
    "\n",
    "- `logit_bias: dict` A dictionary of logit biases to apply to the tokens. This is useful when you want to guide the model towards generating certain types of responses.\n",
    "\n",
    "- `response_format: str` The format of the response. We will cover this later...\n",
    "\n",
    "- `stream: bool = False` Whether to stream the response back to the client. This is useful when you want to get the response in real-time. Nobody likes to sit and wait for a response. Seeing the text generated as and when it is ready is a much better user experience.\n",
    "\n",
    "For a full list of arguments, check out the [OpenAI API documentation](https://platform.openai.com/docs/api-reference/chat/create)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Available models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have used model `gpt-4o-mini`, but there are a range of models available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(id='gpt-3.5-turbo', created=1677610602, object='model', owned_by='openai')\n",
      "Model(id='gpt-3.5-turbo-0125', created=1706048358, object='model', owned_by='system')\n",
      "Model(id='dall-e-2', created=1698798177, object='model', owned_by='system')\n",
      "Model(id='gpt-4-1106-preview', created=1698957206, object='model', owned_by='system')\n",
      "Model(id='tts-1-hd-1106', created=1699053533, object='model', owned_by='system')\n",
      "Model(id='tts-1-hd', created=1699046015, object='model', owned_by='system')\n",
      "Model(id='dall-e-3', created=1698785189, object='model', owned_by='system')\n",
      "Model(id='whisper-1', created=1677532384, object='model', owned_by='openai-internal')\n",
      "Model(id='text-embedding-3-large', created=1705953180, object='model', owned_by='system')\n",
      "Model(id='text-embedding-3-small', created=1705948997, object='model', owned_by='system')\n",
      "Model(id='text-embedding-ada-002', created=1671217299, object='model', owned_by='openai-internal')\n",
      "Model(id='gpt-4-turbo', created=1712361441, object='model', owned_by='system')\n",
      "Model(id='gpt-4o-2024-05-13', created=1715368132, object='model', owned_by='system')\n",
      "Model(id='gpt-4-0125-preview', created=1706037612, object='model', owned_by='system')\n",
      "Model(id='gpt-4-turbo-2024-04-09', created=1712601677, object='model', owned_by='system')\n",
      "Model(id='gpt-4-turbo-preview', created=1706037777, object='model', owned_by='system')\n",
      "Model(id='gpt-3.5-turbo-16k', created=1683758102, object='model', owned_by='openai-internal')\n",
      "Model(id='gpt-4o', created=1715367049, object='model', owned_by='system')\n",
      "Model(id='tts-1', created=1681940951, object='model', owned_by='openai-internal')\n",
      "Model(id='gpt-3.5-turbo-1106', created=1698959748, object='model', owned_by='system')\n",
      "Model(id='tts-1-1106', created=1699053241, object='model', owned_by='system')\n",
      "Model(id='gpt-3.5-turbo-instruct-0914', created=1694122472, object='model', owned_by='system')\n",
      "Model(id='gpt-4', created=1687882411, object='model', owned_by='openai')\n",
      "Model(id='gpt-4-0613', created=1686588896, object='model', owned_by='openai')\n",
      "Model(id='gpt-3.5-turbo-instruct', created=1692901427, object='model', owned_by='system')\n",
      "Model(id='chatgpt-4o-latest', created=1723515131, object='model', owned_by='system')\n",
      "Model(id='babbage-002', created=1692634615, object='model', owned_by='system')\n",
      "Model(id='davinci-002', created=1692634301, object='model', owned_by='system')\n",
      "Model(id='gpt-4o-mini-2024-07-18', created=1721172717, object='model', owned_by='system')\n",
      "Model(id='gpt-4o-mini', created=1721172741, object='model', owned_by='system')\n",
      "Model(id='gpt-4o-2024-08-06', created=1722814719, object='model', owned_by='system')\n"
     ]
    }
   ],
   "source": [
    "for model in client.models.list():\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of writing `gpt-4o-2024-08-06` is the current best offering. But we'll stick with `gpt-4o-mini`, because it is cheaper and still highly capable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The response object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the `response` object?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatCompletion</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'chatcmpl-A5yRAL35y156KgXeY6uUBbLNHD4qh'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">choices</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Choice</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">finish_reason</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">index</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">logprobs</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">message</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatCompletionMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Silent pawsteps glide,  \\nMoonlight dances on the bladeâ€”  \\nFeline honor blinds.  '</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">refusal</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">role</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">function_call</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">tool_calls</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">created</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1725987324</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'gpt-4o-mini-2024-07-18'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">object</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'chat.completion'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">service_tier</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">system_fingerprint</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'fp_483d39d857'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">usage</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">CompletionUsage</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">completion_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span>, <span style=\"color: #808000; text-decoration-color: #808000\">prompt_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">37</span>, <span style=\"color: #808000; text-decoration-color: #808000\">total_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">57</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mChatCompletion\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mid\u001b[0m=\u001b[32m'chatcmpl-A5yRAL35y156KgXeY6uUBbLNHD4qh'\u001b[0m,\n",
       "    \u001b[33mchoices\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mChoice\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mfinish_reason\u001b[0m=\u001b[32m'stop'\u001b[0m,\n",
       "            \u001b[33mindex\u001b[0m=\u001b[1;36m0\u001b[0m,\n",
       "            \u001b[33mlogprobs\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[33mmessage\u001b[0m=\u001b[1;35mChatCompletionMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mcontent\u001b[0m=\u001b[32m'Silent pawsteps glide,  \\nMoonlight dances on the bladeâ€”  \\nFeline honor blinds.  '\u001b[0m,\n",
       "                \u001b[33mrefusal\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                \u001b[33mrole\u001b[0m=\u001b[32m'assistant'\u001b[0m,\n",
       "                \u001b[33mfunction_call\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                \u001b[33mtool_calls\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "            \u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[33mcreated\u001b[0m=\u001b[1;36m1725987324\u001b[0m,\n",
       "    \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o-mini-2024-07-18'\u001b[0m,\n",
       "    \u001b[33mobject\u001b[0m=\u001b[32m'chat.completion'\u001b[0m,\n",
       "    \u001b[33mservice_tier\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[33msystem_fingerprint\u001b[0m=\u001b[32m'fp_483d39d857'\u001b[0m,\n",
       "    \u001b[33musage\u001b[0m=\u001b[1;35mCompletionUsage\u001b[0m\u001b[1m(\u001b[0m\u001b[33mcompletion_tokens\u001b[0m=\u001b[1;36m20\u001b[0m, \u001b[33mprompt_tokens\u001b[0m=\u001b[1;36m37\u001b[0m, \u001b[33mtotal_tokens\u001b[0m=\u001b[1;36m57\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is some useful stuff in here, apart from the `content` property, such as the token usage. You might notice some other things too, like `function_call` and `tool_calls`. These are specific to OpenAI models, and not every model supports function calling or tools, so we won't cover them. We can achieve many of the same effects without them anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming a response\n",
    "\n",
    "Streaming a response is mainly for user experience. It allows the user to see the response as it comes in, rather than waiting for the whole response to come in. For many applications, this might not be necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silent paws in dusk,  \n",
      "Moonlit blade gleams in the nightâ€”  \n",
      "Fierce heart, whiskers twitch."
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_query},\n",
    "  ],\n",
    "  max_tokens=128,\n",
    "  stream=True\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All this really does is create a streaming object, which acts like a generator. We can then print the chunk as it comes in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision input\n",
    "A huge draw of OpenAI models is the ability to input vision data. This is useful for a wide range of applications, including:\n",
    "- Image captioning\n",
    "- Object detection\n",
    "- Face recognition\n",
    "- Image generation\n",
    "\n",
    "Let's try an example of inputting an image. First we need to look at the image:\n",
    "\n",
    "![plot](../imgs/figure.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the caption from this figure:\n",
    "\n",
    "<table><tr><td>\n",
    "\n",
    "**Fig. 2 Spatial and temporal self-similarity and correlation in switching activity.**\n",
    "\n",
    "_(A) Percolating devices produce complex patterns of switching events that are self-similar in nature. The top panel contains 2400 s of data, with the bottom panels showing segments of the data with 10, 100, and 1000 times greater temporal magnification and with 3, 9, and 27 times greater magnification on the vertical scale (units of G0 = 2e2/h, the quantum of conductance, are used for convenience). The activity patterns appear qualitatively similar on multiple different time scales. (B and E) The probability density function (PDF) for changes in total network conductance, P(Î”G), resulting from switching activity exhibits heavy-tailed probability distributions. (C and F) IEIs follow power law distributions, suggestive of correlations between events. (D and G) Further evidence of temporal correlation between events is given by the autocorrelation function (ACF) of the switching activity (red), which decays as a power law over several decades. When the IEI sequence is shuffled (gray), the correlations between events are destroyed, resulting in a significant increase in slope in the ACF. The data shown in (B) to (D) (sample I) were obtained with our standard (slow) sampling rate, and the data shown in (E) to (G) (sample II) were measured 1000 times faster (see Materials and Methods), providing further evidence for self-similarity._\n",
    "</td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This figure is taken from _[Avalanches and criticality in self-organized nanoscale networks, Mallinson et al., 2019.](https://www.science.org/doi/10.1126/sciadv.aaw8438)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use the OpenAI vision model to generate a caption for this figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = (\n",
    "    \"This figure is a caption from a paper entitled Avalanches and criticality in self-organized nanoscale networks. \"\n",
    "    \"Please provide a caption for this figure. \"\n",
    "    \"You should describe the figure, grouping the panels where appropriate. \"\n",
    "    \"Feel free to make any inferences you need to.\"\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of calling a vision model is a little more involved, but OpenAI have a [convenient tutorial](https://platform.openai.com/docs/guides/vision) on how to do this.\n",
    "\n",
    "Essentially we need to first convert the image to a base64 string. We can then pass this to the OpenAI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "  with open(image_path, \"rb\") as image_file:\n",
    "    return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "# Path to your image\n",
    "image_path = \"../imgs/figure.jpeg\"\n",
    "\n",
    "\n",
    "def get_image_caption(image_path, prompt):\n",
    "  # Getting the base64 string\n",
    "  base64_image = encode_image(image_path)\n",
    "\n",
    "  response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": prompt,\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"},\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "  return response.choices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Figure Caption:** \n",
      "\n",
      "This figure presents data illustrating the dynamics of avalanches in self-organized nanoscale networks. \n",
      "\n",
      "**Panel A:** Time series of changes in conductance (Î”G) normalized to a reference conductance (Gâ‚€) are shown across four sub-panels. Each time series displays distinct behaviors over varying time scales: the top panel (100 s) shows broader fluctuations, while the bottom panel (0.03) reflects more stable dynamics, revealing variations at different temporal resolutions.\n",
      "\n",
      "**Panels B and E:** These panels depict the probability distributions \\( P(Î”G) \\) of the magnitude of conductance changes across two different time scales, revealing power-law behavior characterized by fitted exponents (Î’ = -2.59 and -2.36), indicating the heavy-tailed nature of the avalanche events.\n",
      "\n",
      "**Panels C and F:** Present the time-dependent probability \\( P(t) \\) for the two experimental conditions, demonstrating a power-law decay with fitted exponents of \\( t = -1.39 \\) and \\( t = -1.30 \\), respectively, suggesting criticality in the dynamics.\n",
      "\n",
      "**Panels D and G:** These panels show the avalanche size distribution \\( A(t) \\) over time, with results further suggesting distinct scaling behaviors marked by fitted exponents \\( t = -0.19, -0.66 \\) for the first set and \\( t = -0.23, -0.64 \\) for the second, highlighting the complex interactions and growth patterns in the nanoscale networks. \n",
      "\n",
      "Collectively, this figure emphasizes the self-organizing features and critical states of nanoscale networks affected by avalanche dynamics.\n"
     ]
    }
   ],
   "source": [
    "caption = get_image_caption(image_path, prompt)\n",
    "print(caption.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I mean, I don't know about you, but I think that's incredible. Let's consider what it has done:\n",
    "- Correctly grouped the panels in the same way the real caption did.\n",
    "- Provided information on the observation periods.\n",
    "- Drawn out the important information, such as critical exponents.\n",
    "- Made the link between power law distributions and scale-free behaviour.\n",
    "\n",
    "However, it has failed to provide information on temporal correlations, and it has not noticed the self-similarity in caption 1.\n",
    "\n",
    "But this is still quite impressive, and with more information we could potentially get some better captions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools\n",
    "We can also give the model some tools to use - these are essentially just functions that we can call, and the role of the LLM is to generate the arguments to that function.\n",
    "\n",
    "Here is a simple example to do some maths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1382976"
     ]
    }
   ],
   "source": [
    "system_prompt = \"You are a helpful mathematician. You will only solve the problems given to you. Do not provide any additional information. Provide only the answer.\"\n",
    "user_query = \"What is 1056 * 1316?\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_query},\n",
    "  ],\n",
    "  max_tokens=256,\n",
    "  stream=True\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1389696"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1056 * 1316"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the LLM is not correct :(.\n",
    "\n",
    "To endow the model with \"tool use\", we add a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply(a: float, b: float) -> float:\n",
    "    return a * b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then provide the model with a _schema_, which is just a description of the function in dictionary form (or JSON):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_schema = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"multiply\",\n",
    "        \"description\": \"Given two floats, a and b, return the product of a and b.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"a\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"description\": \"The first number to multiply.\"\n",
    "                },\n",
    "                \"b\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"description\": \"The second number to multiply.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"a\", \"b\"],\n",
    "            \"additionalProperties\": False,\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "tools = [\n",
    "    tool_schema\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we make function calls with an LLM, we have to let it know that it has access to one or more tools. We do this by passing in the `tools` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessageToolCall(id='call_1vXSH7jPzjHCLYBlccyzewuB', function=Function(arguments='{\"a\":1056,\"b\":1316}', name='multiply'), type='function')\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_query},\n",
    "    ],\n",
    "    max_tokens=256,\n",
    "    tools=tools,\n",
    "    )\n",
    "\n",
    "print(response.choices[0].message.tool_calls[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now in our `response`, we have this extra part called `tool_calls` that we can extract information from - in this case the arguments to the `multiply` function.\n",
    "\n",
    "Note that you could achieve a similar result with appropriate prompting - e.g. \"Extract only the arguments to a function that multiplies two numbers.\"\n",
    "\n",
    "We unpack the actual arguments as a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1056, 'b': 1316}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "tool_call = response.choices[0].message.tool_calls[0]\n",
    "arguments = json.loads(tool_call.function.arguments)\n",
    "print(arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we now feed the arguments into our `multiply` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1389696\n"
     ]
    }
   ],
   "source": [
    "result = multiply(**arguments)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have the answer. We can either just return this, or we can feed it back into the LLM. We need to provide the model with the `tool_calls[0].id`, so that it can associate response messages of the tool type with the correct tool call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1389696'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_call_result = {\n",
    "    \"role\": \"tool\",\n",
    "    \"content\": json.dumps({\n",
    "        \"a\" : arguments[\"a\"],\n",
    "        \"b\" : arguments[\"b\"],\n",
    "        \"result\": result\n",
    "    }),\n",
    "    \"tool_call_id\": response.choices[0].message.tool_calls[0].id\n",
    "}\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_query},\n",
    "        response.choices[0].message,\n",
    "        tool_call_result\n",
    "    ],\n",
    "    max_tokens=56\n",
    ")\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is quite a lot of work to multiply two numbers, but of course the power comes when doing more complex tasks.\n",
    "\n",
    "And this brings to light an interesting contrast. People talk a lot about \"agents\" and \"tools\" and \"systems\", and when we interact with ChatGPT, we get a single coherent experience. Sometimes it is difficult to distinguish between what the LLM is doing, and what the software engineers have built around it in order to create this seemless experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brave_search import BraveSearchWrapper, scrape_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write two little \"wrapper\" functions that will abstract away all the details and formate the responses from the web search for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BRAVE_API_KEY = os.getenv(\"BRAVE_API_KEY\")\n",
    "\n",
    "brave_client = BraveSearchWrapper(\n",
    "            api_key=BRAVE_API_KEY,\n",
    "            search_kwargs={},\n",
    "        )\n",
    "\n",
    "\n",
    "def search_brave(query: str, **kwargs):\n",
    "    response = brave_client.download_documents(query, **kwargs)\n",
    "    \n",
    "    # format the response of the top 5 results\n",
    "    formatted_response = \"\"\n",
    "    for result in response[:5]:\n",
    "        formatted_response += f\"{result.metadata['title']}\\n\"\n",
    "        formatted_response += f\"{result.metadata['link']}\\n\\n\"\n",
    "\n",
    "    return formatted_response\n",
    "\n",
    "def scrape_content(url: str):\n",
    "    return scrape_url(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_results = search_brave(\"Best pumpkin pie recipe\")\n",
    "print(best_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we click on any of these links, we can see that they do indeed work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have two tools that the model can interact with - the Brave web API, and the scraper tool, that actually gets the text from the internet. As a reminder, we have to define special JSON that tells the model how to interact with the tool. This is the `tool.json` file, and loaded below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from rich.pretty import pprint\n",
    "\n",
    "tools = json.load(open('tools.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   </span><span style=\"font-weight: bold\">{</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   â”‚   </span><span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'function'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   â”‚   </span><span style=\"color: #008000; text-decoration-color: #008000\">'function'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   â”‚   â”‚   </span><span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'search_brave'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   â”‚   â”‚   </span><span style=\"color: #008000; text-decoration-color: #008000\">'description'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Search the internet using the Brave search engine'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   â”‚   â”‚   </span><span style=\"color: #008000; text-decoration-color: #008000\">'parameters'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   â”‚   â”‚   â”‚   </span><span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'object'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   â”‚   â”‚   â”‚   </span><span style=\"color: #008000; text-decoration-color: #008000\">'properties'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   â”‚   â”‚   â”‚   â”‚   </span><span style=\"color: #008000; text-decoration-color: #008000\">'query'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'string'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'description'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'The query to send to the search engine'</span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   â”‚   â”‚   â”‚   </span><span style=\"font-weight: bold\">}</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   â”‚   â”‚   â”‚   </span><span style=\"color: #008000; text-decoration-color: #008000\">'required'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'query'</span><span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   â”‚   â”‚   </span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   â”‚   </span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   </span><span style=\"font-weight: bold\">}</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   </span><span style=\"font-weight: bold\">{</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   â”‚   </span><span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'function'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   â”‚   </span><span style=\"color: #008000; text-decoration-color: #008000\">'function'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   â”‚   â”‚   </span><span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'scrape_content'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   â”‚   â”‚   </span><span style=\"color: #008000; text-decoration-color: #008000\">'description'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Scrape the text content from a website'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   â”‚   â”‚   </span><span style=\"color: #008000; text-decoration-color: #008000\">'parameters'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   â”‚   â”‚   â”‚   </span><span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'object'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   â”‚   â”‚   â”‚   </span><span style=\"color: #008000; text-decoration-color: #008000\">'properties'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'url'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'string'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'description'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'The website url'</span><span style=\"font-weight: bold\">}}</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   â”‚   â”‚   â”‚   </span><span style=\"color: #008000; text-decoration-color: #008000\">'required'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'url'</span><span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   â”‚   â”‚   </span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   â”‚   </span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   </span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "\u001b[2;32mâ”‚   \u001b[0m\u001b[1m{\u001b[0m\n",
       "\u001b[2;32mâ”‚   â”‚   \u001b[0m\u001b[32m'type'\u001b[0m: \u001b[32m'function'\u001b[0m,\n",
       "\u001b[2;32mâ”‚   â”‚   \u001b[0m\u001b[32m'function'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "\u001b[2;32mâ”‚   â”‚   â”‚   \u001b[0m\u001b[32m'name'\u001b[0m: \u001b[32m'search_brave'\u001b[0m,\n",
       "\u001b[2;32mâ”‚   â”‚   â”‚   \u001b[0m\u001b[32m'description'\u001b[0m: \u001b[32m'Search the internet using the Brave search engine'\u001b[0m,\n",
       "\u001b[2;32mâ”‚   â”‚   â”‚   \u001b[0m\u001b[32m'parameters'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "\u001b[2;32mâ”‚   â”‚   â”‚   â”‚   \u001b[0m\u001b[32m'type'\u001b[0m: \u001b[32m'object'\u001b[0m,\n",
       "\u001b[2;32mâ”‚   â”‚   â”‚   â”‚   \u001b[0m\u001b[32m'properties'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "\u001b[2;32mâ”‚   â”‚   â”‚   â”‚   â”‚   \u001b[0m\u001b[32m'query'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'type'\u001b[0m: \u001b[32m'string'\u001b[0m, \u001b[32m'description'\u001b[0m: \u001b[32m'The query to send to the search engine'\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[2;32mâ”‚   â”‚   â”‚   â”‚   \u001b[0m\u001b[1m}\u001b[0m,\n",
       "\u001b[2;32mâ”‚   â”‚   â”‚   â”‚   \u001b[0m\u001b[32m'required'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'query'\u001b[0m\u001b[1m]\u001b[0m\n",
       "\u001b[2;32mâ”‚   â”‚   â”‚   \u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[2;32mâ”‚   â”‚   \u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[2;32mâ”‚   \u001b[0m\u001b[1m}\u001b[0m,\n",
       "\u001b[2;32mâ”‚   \u001b[0m\u001b[1m{\u001b[0m\n",
       "\u001b[2;32mâ”‚   â”‚   \u001b[0m\u001b[32m'type'\u001b[0m: \u001b[32m'function'\u001b[0m,\n",
       "\u001b[2;32mâ”‚   â”‚   \u001b[0m\u001b[32m'function'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "\u001b[2;32mâ”‚   â”‚   â”‚   \u001b[0m\u001b[32m'name'\u001b[0m: \u001b[32m'scrape_content'\u001b[0m,\n",
       "\u001b[2;32mâ”‚   â”‚   â”‚   \u001b[0m\u001b[32m'description'\u001b[0m: \u001b[32m'Scrape the text content from a website'\u001b[0m,\n",
       "\u001b[2;32mâ”‚   â”‚   â”‚   \u001b[0m\u001b[32m'parameters'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "\u001b[2;32mâ”‚   â”‚   â”‚   â”‚   \u001b[0m\u001b[32m'type'\u001b[0m: \u001b[32m'object'\u001b[0m,\n",
       "\u001b[2;32mâ”‚   â”‚   â”‚   â”‚   \u001b[0m\u001b[32m'properties'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'url'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'type'\u001b[0m: \u001b[32m'string'\u001b[0m, \u001b[32m'description'\u001b[0m: \u001b[32m'The website url'\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m,\n",
       "\u001b[2;32mâ”‚   â”‚   â”‚   â”‚   \u001b[0m\u001b[32m'required'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'url'\u001b[0m\u001b[1m]\u001b[0m\n",
       "\u001b[2;32mâ”‚   â”‚   â”‚   \u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[2;32mâ”‚   â”‚   \u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[2;32mâ”‚   \u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import ChatModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \"You are a helpful assistant that can provide information on a wide range of topics. \"\n",
    "    \"If you feel necessary, you can use two tools to help you find information: Brave Search and a web scraper. \"\n",
    "    \"If the user requests information that might be better found on the web, you can use these tools to help you. \"\n",
    "    \"If you need to use these tools, first provide only the titles and links from the Brave Search results, \"\n",
    "    \"and then clarify with the user if they would like more information. \"\n",
    "    \"If they require more information, you can use the web scraper and then provide a summary of the content. \"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our system prompt is simple:\n",
    "\n",
    "---\n",
    "```jinja\n",
    "You are a helpful assistant that can provide information on a wide range of topics. If you feel necessary, you can use two tools to help you find information: Brave Search and a web scraper. If the user requests information that might be better found on the web, you can use these tools to help you. If you need to use these tools, first provide only the titles and links from the Brave Search results, and then clarify with the user if they would like more information. If they require more information, you can use the web scraper and then provide a summary of the content. \n",
    "```\n",
    "---\n",
    "\n",
    "We are essentially giving the model two options - it can use the Brave search tool to find information, and then if the user wants more information, it can use the scraper tool to get the text from the web page, and summarize it. This is a very basic example of using the LLM as a router to determine which tool to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: below we import our ChatModel class from the `models.py` file. Before you run the code, please make sure you've added your API key to the `models.py` file, and saved the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import ChatModel\n",
    "\n",
    "tool_system_prompt = template_manager.render('tool_prompt.jinja')\n",
    "model = ChatModel('gpt-4o-mini', system_prompt=system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.generate(\n",
    "    \"What is the best pumpkin pie recipe?\",\n",
    "    max_tokens=512,\n",
    "    temperature=0.5,\n",
    "    tools=tools,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model has not actually produced a result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No response generated.\n"
     ]
    }
   ],
   "source": [
    "if not response.choices[0].message.content:\n",
    "    print(\"No response generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But don't panic - if we look at the `ChatCompletion` object, there is no message content, and the reason for the stoppage is due to a `tool_call`. We can also see that we have an additional `ChatCompletionMessageToolCall` object. So we need to extract the appropriate information and pass it to the appropriate tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatCompletion</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   </span><span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'chatcmpl-AWKRgn1mifgnMjmcebG6t3C9hpoC6'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   </span><span style=\"color: #808000; text-decoration-color: #808000\">choices</span>=<span style=\"font-weight: bold\">[</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   â”‚   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Choice</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   â”‚   â”‚   </span><span style=\"color: #808000; text-decoration-color: #808000\">finish_reason</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'tool_calls'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   â”‚   â”‚   </span><span style=\"color: #808000; text-decoration-color: #808000\">index</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   â”‚   â”‚   </span><span style=\"color: #808000; text-decoration-color: #808000\">logprobs</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   â”‚   â”‚   </span><span style=\"color: #808000; text-decoration-color: #808000\">message</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatCompletionMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   â”‚   â”‚   â”‚   </span><span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   â”‚   â”‚   â”‚   </span><span style=\"color: #808000; text-decoration-color: #808000\">refusal</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   â”‚   â”‚   â”‚   </span><span style=\"color: #808000; text-decoration-color: #808000\">role</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   â”‚   â”‚   â”‚   </span><span style=\"color: #808000; text-decoration-color: #808000\">audio</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   â”‚   â”‚   â”‚   </span><span style=\"color: #808000; text-decoration-color: #808000\">function_call</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   â”‚   â”‚   â”‚   </span><span style=\"color: #808000; text-decoration-color: #808000\">tool_calls</span>=<span style=\"font-weight: bold\">[</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   â”‚   â”‚   â”‚   â”‚   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatCompletionMessageToolCall</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   </span><span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'call_dQCZGpf0cm7j3S8cYMrp0jQ2'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   </span><span style=\"color: #808000; text-decoration-color: #808000\">function</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Function</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">arguments</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'{\"query\":\"best pumpkin pie recipe\"}'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'search_brave'</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   </span><span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'function'</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   â”‚   â”‚   â”‚   â”‚   </span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   â”‚   â”‚   â”‚   </span><span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   â”‚   â”‚   </span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   â”‚   </span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   </span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   </span><span style=\"color: #808000; text-decoration-color: #808000\">created</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1732268452</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   </span><span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'gpt-4o-mini-2024-07-18'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   </span><span style=\"color: #808000; text-decoration-color: #808000\">object</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'chat.completion'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   </span><span style=\"color: #808000; text-decoration-color: #808000\">service_tier</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   </span><span style=\"color: #808000; text-decoration-color: #808000\">system_fingerprint</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'fp_0705bf87c0'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   </span><span style=\"color: #808000; text-decoration-color: #808000\">usage</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">CompletionUsage</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   â”‚   </span><span style=\"color: #808000; text-decoration-color: #808000\">completion_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   â”‚   </span><span style=\"color: #808000; text-decoration-color: #808000\">prompt_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">211</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   â”‚   </span><span style=\"color: #808000; text-decoration-color: #808000\">total_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">229</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   â”‚   </span><span style=\"color: #808000; text-decoration-color: #808000\">completion_tokens_details</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">CompletionTokensDetails</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   â”‚   â”‚   </span><span style=\"color: #808000; text-decoration-color: #808000\">accepted_prediction_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   â”‚   â”‚   </span><span style=\"color: #808000; text-decoration-color: #808000\">audio_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   â”‚   â”‚   </span><span style=\"color: #808000; text-decoration-color: #808000\">reasoning_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   â”‚   â”‚   </span><span style=\"color: #808000; text-decoration-color: #808000\">rejected_prediction_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   â”‚   </span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   â”‚   </span><span style=\"color: #808000; text-decoration-color: #808000\">prompt_tokens_details</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PromptTokensDetails</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">audio_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">cached_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   </span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mChatCompletion\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32mâ”‚   \u001b[0m\u001b[33mid\u001b[0m=\u001b[32m'chatcmpl-AWKRgn1mifgnMjmcebG6t3C9hpoC6'\u001b[0m,\n",
       "\u001b[2;32mâ”‚   \u001b[0m\u001b[33mchoices\u001b[0m=\u001b[1m[\u001b[0m\n",
       "\u001b[2;32mâ”‚   â”‚   \u001b[0m\u001b[1;35mChoice\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32mâ”‚   â”‚   â”‚   \u001b[0m\u001b[33mfinish_reason\u001b[0m=\u001b[32m'tool_calls'\u001b[0m,\n",
       "\u001b[2;32mâ”‚   â”‚   â”‚   \u001b[0m\u001b[33mindex\u001b[0m=\u001b[1;36m0\u001b[0m,\n",
       "\u001b[2;32mâ”‚   â”‚   â”‚   \u001b[0m\u001b[33mlogprobs\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "\u001b[2;32mâ”‚   â”‚   â”‚   \u001b[0m\u001b[33mmessage\u001b[0m=\u001b[1;35mChatCompletionMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32mâ”‚   â”‚   â”‚   â”‚   \u001b[0m\u001b[33mcontent\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "\u001b[2;32mâ”‚   â”‚   â”‚   â”‚   \u001b[0m\u001b[33mrefusal\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "\u001b[2;32mâ”‚   â”‚   â”‚   â”‚   \u001b[0m\u001b[33mrole\u001b[0m=\u001b[32m'assistant'\u001b[0m,\n",
       "\u001b[2;32mâ”‚   â”‚   â”‚   â”‚   \u001b[0m\u001b[33maudio\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "\u001b[2;32mâ”‚   â”‚   â”‚   â”‚   \u001b[0m\u001b[33mfunction_call\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "\u001b[2;32mâ”‚   â”‚   â”‚   â”‚   \u001b[0m\u001b[33mtool_calls\u001b[0m=\u001b[1m[\u001b[0m\n",
       "\u001b[2;32mâ”‚   â”‚   â”‚   â”‚   â”‚   \u001b[0m\u001b[1;35mChatCompletionMessageToolCall\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32mâ”‚   â”‚   â”‚   â”‚   â”‚   â”‚   \u001b[0m\u001b[33mid\u001b[0m=\u001b[32m'call_dQCZGpf0cm7j3S8cYMrp0jQ2'\u001b[0m,\n",
       "\u001b[2;32mâ”‚   â”‚   â”‚   â”‚   â”‚   â”‚   \u001b[0m\u001b[33mfunction\u001b[0m=\u001b[1;35mFunction\u001b[0m\u001b[1m(\u001b[0m\u001b[33marguments\u001b[0m=\u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"query\":\"best pumpkin pie recipe\"\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m, \u001b[33mname\u001b[0m=\u001b[32m'search_brave'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32mâ”‚   â”‚   â”‚   â”‚   â”‚   â”‚   \u001b[0m\u001b[33mtype\u001b[0m=\u001b[32m'function'\u001b[0m\n",
       "\u001b[2;32mâ”‚   â”‚   â”‚   â”‚   â”‚   \u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32mâ”‚   â”‚   â”‚   â”‚   \u001b[0m\u001b[1m]\u001b[0m\n",
       "\u001b[2;32mâ”‚   â”‚   â”‚   \u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32mâ”‚   â”‚   \u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32mâ”‚   \u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32mâ”‚   \u001b[0m\u001b[33mcreated\u001b[0m=\u001b[1;36m1732268452\u001b[0m,\n",
       "\u001b[2;32mâ”‚   \u001b[0m\u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o-mini-2024-07-18'\u001b[0m,\n",
       "\u001b[2;32mâ”‚   \u001b[0m\u001b[33mobject\u001b[0m=\u001b[32m'chat.completion'\u001b[0m,\n",
       "\u001b[2;32mâ”‚   \u001b[0m\u001b[33mservice_tier\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "\u001b[2;32mâ”‚   \u001b[0m\u001b[33msystem_fingerprint\u001b[0m=\u001b[32m'fp_0705bf87c0'\u001b[0m,\n",
       "\u001b[2;32mâ”‚   \u001b[0m\u001b[33musage\u001b[0m=\u001b[1;35mCompletionUsage\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32mâ”‚   â”‚   \u001b[0m\u001b[33mcompletion_tokens\u001b[0m=\u001b[1;36m18\u001b[0m,\n",
       "\u001b[2;32mâ”‚   â”‚   \u001b[0m\u001b[33mprompt_tokens\u001b[0m=\u001b[1;36m211\u001b[0m,\n",
       "\u001b[2;32mâ”‚   â”‚   \u001b[0m\u001b[33mtotal_tokens\u001b[0m=\u001b[1;36m229\u001b[0m,\n",
       "\u001b[2;32mâ”‚   â”‚   \u001b[0m\u001b[33mcompletion_tokens_details\u001b[0m=\u001b[1;35mCompletionTokensDetails\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32mâ”‚   â”‚   â”‚   \u001b[0m\u001b[33maccepted_prediction_tokens\u001b[0m=\u001b[1;36m0\u001b[0m,\n",
       "\u001b[2;32mâ”‚   â”‚   â”‚   \u001b[0m\u001b[33maudio_tokens\u001b[0m=\u001b[1;36m0\u001b[0m,\n",
       "\u001b[2;32mâ”‚   â”‚   â”‚   \u001b[0m\u001b[33mreasoning_tokens\u001b[0m=\u001b[1;36m0\u001b[0m,\n",
       "\u001b[2;32mâ”‚   â”‚   â”‚   \u001b[0m\u001b[33mrejected_prediction_tokens\u001b[0m=\u001b[1;36m0\u001b[0m\n",
       "\u001b[2;32mâ”‚   â”‚   \u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32mâ”‚   â”‚   \u001b[0m\u001b[33mprompt_tokens_details\u001b[0m=\u001b[1;35mPromptTokensDetails\u001b[0m\u001b[1m(\u001b[0m\u001b[33maudio_tokens\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mcached_tokens\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32mâ”‚   \u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_call = response.choices[0].message.tool_calls[0]\n",
    "tool_id = tool_call.id\n",
    "arguments = json.loads(tool_call.function.arguments)\n",
    "query = arguments['query']\n",
    "\n",
    "result = search_brave(query)\n",
    "tool_response = {'role':'tool', 'content': result, 'tool_id': tool_id}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have formatted the next chat message to be a `'tool'` role rather than a `'user'` or `'assistant'` role. Note the `'tool_id'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   </span><span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'tool'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   </span><span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"The BEST Pumpkin Pie Recipe - Tastes Better From Scratch\\nhttps://tastesbetterfromscratch.com/pumpkin-pie-with-caramel-pecan-topping/\\n\\nPumpkin Pie Recipe - Preppy Kitchen\\nhttps://preppykitchen.com/pumpkin-pie-2/\\n\\nHomemade Fresh Pumpkin Pie Recipe\\nhttps://www.allrecipes.com/recipe/13711/homemade-fresh-pumpkin-pie/\\n\\nThe Great Pumpkin Pie Recipe - Sally's Baking Addiction\\nhttps://sallysbakingaddiction.com/the-great-pumpkin-pie-recipe/\\n\\nr/thanksgiving on Reddit: What's a good pumpkin pie recipe?\\nhttps://www.reddit.com/r/thanksgiving/comments/1fvxp6q/whats_a_good_pumpkin_pie_recipe/\\n\\n\"</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â”‚   </span><span style=\"color: #008000; text-decoration-color: #008000\">'tool_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'call_dQCZGpf0cm7j3S8cYMrp0jQ2'</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "\u001b[2;32mâ”‚   \u001b[0m\u001b[32m'role'\u001b[0m: \u001b[32m'tool'\u001b[0m,\n",
       "\u001b[2;32mâ”‚   \u001b[0m\u001b[32m'content'\u001b[0m: \u001b[32m\"The BEST Pumpkin Pie Recipe - Tastes Better From Scratch\\nhttps://tastesbetterfromscratch.com/pumpkin-pie-with-caramel-pecan-topping/\\n\\nPumpkin Pie Recipe - Preppy Kitchen\\nhttps://preppykitchen.com/pumpkin-pie-2/\\n\\nHomemade Fresh Pumpkin Pie Recipe\\nhttps://www.allrecipes.com/recipe/13711/homemade-fresh-pumpkin-pie/\\n\\nThe Great Pumpkin Pie Recipe - Sally's Baking Addiction\\nhttps://sallysbakingaddiction.com/the-great-pumpkin-pie-recipe/\\n\\nr/thanksgiving on Reddit: What's a good pumpkin pie recipe?\\nhttps://www.reddit.com/r/thanksgiving/comments/1fvxp6q/whats_a_good_pumpkin_pie_recipe/\\n\\n\"\u001b[0m,\n",
       "\u001b[2;32mâ”‚   \u001b[0m\u001b[32m'tool_id'\u001b[0m: \u001b[32m'call_dQCZGpf0cm7j3S8cYMrp0jQ2'\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(tool_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first add some additional functionality to our `ChatModel` class so that it can interact with tools. The key section here is that we have to have the following order:\n",
    "\n",
    "```text\n",
    "[\n",
    "    {\n",
    "        User message,\n",
    "        Tool completion,\n",
    "        Tool message (as above),\n",
    "    }\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the below is running, we should try to trace through exatly what is happening each step of the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatModel:\n",
    "    def __init__(self, model: str, system_prompt: str = None):\n",
    "        self.model = model\n",
    "        self.client = OpenAI(api_key = api_key)\n",
    "        self.chat_history: list[dict[str, str]] = []\n",
    "        \n",
    "        if system_prompt:\n",
    "            self.add_message(\"system\", system_prompt)\n",
    "\n",
    "    def add_message(self, role: str, content: str) -> None:\n",
    "        self.chat_history.append({\n",
    "            \"role\": role,\n",
    "            \"content\": content,\n",
    "        })\n",
    "\n",
    "    def clear_history(self) -> None:\n",
    "        self.chat_history = []\n",
    "\n",
    "    def get_history(self) -> list[dict[str, str]]:\n",
    "        return self.chat_history\n",
    "    \n",
    "    def tool_call(self, tool_call) -> dict:\n",
    "        tool_id = tool_call.id\n",
    "        arguments = json.loads(tool_call.function.arguments)\n",
    "\n",
    "        if tool_call.function.name == 'search_brave':\n",
    "            print(\"Searching for information...\")\n",
    "            query = arguments['query']\n",
    "            result = search_brave(query)\n",
    "\n",
    "        elif tool_call.function.name == 'scrape_content':\n",
    "            print(\"Scraping content...\")\n",
    "            url = arguments['url']\n",
    "            result = scrape_content(url)\n",
    "            \n",
    "        tool_response = {'role':'tool', 'content': result, 'tool_call_id': tool_id}\n",
    "\n",
    "        return tool_response\n",
    "    \n",
    "\n",
    "    def format_tool_response(self, tool_call):\n",
    "        formatted_dict = {\n",
    "        \"role\": \"assistant\",\n",
    "        \"tool_calls\": [\n",
    "                {\n",
    "                    \"id\": tool_call.id,\n",
    "                    \"type\": tool_call.type,\n",
    "                    \"function\": {\n",
    "                        \"arguments\": tool_call.function.arguments,\n",
    "                        \"name\": tool_call.function.name\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        return formatted_dict\n",
    "    \n",
    "\n",
    "    def generate(self, message: str, **kwargs) -> str:\n",
    "        self.add_message(\"user\", message)\n",
    "        \n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=self.chat_history,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        if not response.choices[0].message.content:\n",
    "            \n",
    "\n",
    "            tool_completion = response.choices[0].message.tool_calls[0]\n",
    "            tool_response = self.tool_call(tool_completion)\n",
    "\n",
    "            messages = self.chat_history + [self.format_tool_response(tool_completion)] + [tool_response]\n",
    "\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=messages,\n",
    "                **kwargs\n",
    "            )\n",
    "\n",
    "        assistant_message = response.choices[0].message.content\n",
    "        self.add_message(\"assistant\", assistant_message)\n",
    "    \n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatModel('gpt-4o-mini', system_prompt=system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for information...\n",
      "Here are some highly-rated pumpkin pie recipes:\n",
      "\n",
      "1. [The BEST Pumpkin Pie Recipe - Tastes Better From Scratch](https://tastesbetterfromscratch.com/pumpkin-pie-with-caramel-pecan-topping/)\n",
      "2. [Pumpkin Pie Recipe - Preppy Kitchen](https://preppykitchen.com/pumpkin-pie-2/)\n",
      "3. [Homemade Fresh Pumpkin Pie Recipe - Allrecipes](https://www.allrecipes.com/recipe/13711/homemade-fresh-pumpkin-pie/)\n",
      "4. [The Great Pumpkin Pie Recipe - Sally's Baking Addiction](https://sallysbakingaddiction.com/the-great-pumpkin-pie-recipe/)\n",
      "5. [r/thanksgiving on Reddit: What's a good pumpkin pie recipe?](https://www.reddit.com/r/thanksgiving/comments/1fvxp6q/whats_a_good_pumpkin_pie_recipe/)\n",
      "\n",
      "Would you like more information about any specific recipe?\n"
     ]
    }
   ],
   "source": [
    "response = model.generate(\n",
    "    \"What is the best pumpkin pie recipe?\",\n",
    "    max_tokens=512,\n",
    "    temperature=0.5,\n",
    "    tools=tools,\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping content...\n",
      "The recipe for the BEST Pumpkin Pie from Tastes Better From Scratch is a classic Thanksgiving dessert that is both simple and delicious, featuring an optional caramel pecan topping for added flavor. Hereâ€™s a summary of the key points:\n",
      "\n",
      "### Ingredients:\n",
      "- **For the Pie:**\n",
      "  - 1 unbaked 9-inch pie crust (homemade or store-bought)\n",
      "  - 3/4 cup granulated sugar\n",
      "  - Spices: 1 tsp cinnamon, 1/2 tsp salt, 1/2 tsp ginger, 1/4 tsp cloves\n",
      "  - 2 large eggs\n",
      "  - 15 oz can of pumpkin puree (or fresh pumpkin)\n",
      "  - 12 oz can of evaporated milk\n",
      "\n",
      "- **Optional Caramel Pecan Topping:**\n",
      "  - 1/2 cup light brown sugar\n",
      "  - 2 tbsp heavy whipping cream\n",
      "  - 1 tbsp light corn syrup\n",
      "  - 1 tbsp butter\n",
      "  - 1/2 cup chopped pecans\n",
      "  - 1/2 tsp vanilla extract\n",
      "\n",
      "### Instructions:\n",
      "1. **Preparation:**\n",
      "   - Preheat the oven to 425Â°F (220Â°C).\n",
      "   - Beat eggs and pumpkin together in a large bowl. In another bowl, mix sugar and spices, then combine with the pumpkin mixture. Gradually stir in evaporated milk.\n",
      "   \n",
      "2. **Baking:**\n",
      "   - Pour the mixture into the unbaked pie shell. Bake at 425Â°F for 15 minutes, then reduce the temperature to 350Â°F (175Â°C) and bake for an additional 40-50 minutes until set.\n",
      "\n",
      "3. **Topping (Optional):**\n",
      "   - For the caramel pecan topping, combine brown sugar, cream, corn syrup, and butter in a saucepan. Bring to a boil, then simmer for about 5 minutes. Stir in pecans and vanilla.\n",
      "\n",
      "### Tips:\n",
      "- To check doneness, gently jiggle the pie; a slight jiggle in the center is okay as it will continue to set while cooling.\n",
      "- Allow the pie to cool completely to avoid cracks on the surface.\n",
      "- The pie can be made a day in advance and stored in the refrigerator.\n",
      "\n",
      "### Storage:\n",
      "- Homemade pumpkin pie should be refrigerated if not consumed within 2 hours. It can last for 3-4 days in the fridge or be frozen for up to 3 months.\n",
      "\n",
      "This recipe is praised for its traditional flavor and texture, making it a favorite for Thanksgiving gatherings. Would you like\n"
     ]
    }
   ],
   "source": [
    "# Please note this will likely return an error on KATE!\n",
    "response = model.generate(\n",
    "    \"Yes, can you summarize the first link for me please?\",\n",
    "    max_tokens=512,\n",
    "    temperature=0.5,\n",
    "    tools=tools,\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is actually a great summary of the ingredients and recipe.\n",
    "\n",
    "> **Note**: You would probably want to separate your tools from your model, and have a separate class that manages the tools. This would allow you to easily swap out tools, and to manage the tools in a more modular way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also pick a new link, not found by the model, and ask it to scrape that link instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping content...\n",
      "The pumpkin pie recipe from Inspired Taste is celebrated for its simplicity and rich flavor. Hereâ€™s a summary of the key points:\n",
      "\n",
      "### Overview:\n",
      "- This easy homemade pumpkin pie features a flaky crust and a creamy filling made with heavy cream instead of sweetened condensed milk, resulting in a rich and delicious dessert.\n",
      "\n",
      "### Key Ingredients:\n",
      "- **Pie Crust:** Use a homemade or store-bought pie crust. The authors recommend a butter pie crust.\n",
      "- **Pumpkin:** The recipe works with both canned pumpkin puree (like Libby's) and homemade puree.\n",
      "- **Eggs:** Three large eggs are used to help the filling set.\n",
      "- **Sugars:** A mix of granulated and light brown sugar is used for sweetness, which is less than in other recipes.\n",
      "- **Cream:** Heavy cream is used for a smooth texture.\n",
      "- **Spices:** The filling includes vanilla, cinnamon, ginger, ground cloves, and salt, allowing the pumpkin flavor to shine.\n",
      "\n",
      "### Instructions:\n",
      "1. **Prepare the Crust:** Roll out the pie dough and fit it into a 9-inch pie dish, then refrigerate while preparing the filling.\n",
      "2. **Make the Filling:** Whisk together eggs, sugars, pumpkin puree, cream, vanilla, and spices until well blended.\n",
      "3. **Bake the Pie:** \n",
      "   - Preheat the oven to 425Â°F and bake for 15 minutes.\n",
      "   - Reduce the temperature to 375Â°F and bake for an additional 35-45 minutes until set.\n",
      "4. **Cool and Serve:** Allow the pie to cool completely at room temperature, then refrigerate overnight for the best texture.\n",
      "\n",
      "### Tips:\n",
      "- The pie can be made in advance, and refrigerating it overnight improves the custard texture.\n",
      "- Store leftovers in the refrigerator for up to three days or freeze for up to three months.\n",
      "\n",
      "### Conclusion:\n",
      "This recipe is praised for its ease and flavor, making it a perfect choice for Thanksgiving or any fall gathering. The authors also provide alternatives for those who may want to adjust the spices or use pumpkin pie spice instead.\n"
     ]
    }
   ],
   "source": [
    "# Please note this will likely return an error on KATE!\n",
    "\n",
    "new_input = (\n",
    "    \"That's great thanks, but could you also provide me a summary of this link please:\\n\"\n",
    "    \"https://www.inspiredtaste.net/24962/pumpkin-pie-recipe/\"\n",
    "             )\n",
    "\n",
    "response = model.generate(\n",
    "    new_input,\n",
    "    max_tokens=512,\n",
    "    temperature=0.5,\n",
    "    tools=tools,\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both pumpkin pie recipes have their unique strengths, so the choice depends on your preferences:\n",
      "\n",
      "1. **Tastes Better From Scratch:**\n",
      "   - **Pros:** This recipe offers a traditional approach with a classic flavor profile. The addition of a caramel pecan topping adds a delightful twist if you want something extra special. \n",
      "   - **Ideal for:** Those who enjoy a rich, sweet pie with a bit of texture from the pecans and caramel.\n",
      "\n",
      "2. **Inspired Taste:**\n",
      "   - **Pros:** This recipe emphasizes simplicity and uses heavy cream for a smooth, creamy filling. Itâ€™s straightforward and can be made with either canned or homemade pumpkin. \n",
      "   - **Ideal for:** Those who prefer a more traditional pumpkin pie flavor with a silky texture and want a recipe that is easy to follow.\n",
      "\n",
      "### Recommendation:\n",
      "- If youâ€™re looking for a classic pumpkin pie experience with an option for a unique topping, go with **Tastes Better From Scratch**.\n",
      "- If you prefer a simpler recipe that focuses on the creamy texture of the filling, choose **Inspired Taste**.\n",
      "\n",
      "Ultimately, both recipes are excellent, so you can't go wrong with either choice! If you want to impress guests or bring something special, consider the first option with the caramel topping. If you want a reliable, straightforward pie, the second option is perfect.\n"
     ]
    }
   ],
   "source": [
    "# Please note this will likely return an error on KATE!\n",
    "\n",
    "response = model.generate(\n",
    "    \"Of these two options, which one would you recommend?\",\n",
    "    max_tokens=512,\n",
    "    temperature=0.5,\n",
    "    tools=tools,\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
